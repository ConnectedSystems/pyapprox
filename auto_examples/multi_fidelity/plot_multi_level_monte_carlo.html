

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Multi-level Monte Carlo &mdash; PyApprox 1.0.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-dataframe.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"V": ["{\\boldsymbol{#1}}", 1], "mean": ["{\\mathbb{E}\\left[#1\\right]}", 1], "var": ["{\\mathbb{V}\\left[#1\\right]}", 1], "argmin": "{\\mathrm{argmin}}", "rv": "z", "reals": "\\mathbb{R}", "pdf": "\\rho", "rvdom": "\\Gamma", "coloneqq": "\\colon=", "norm": ["{\\lVert #1 \\rVert}", 1], "argmax": ["\\operatorname{argmax}"], "covar": ["\\mathbb{C}\\text{ov}\\left[#1,#2\\right]", 2], "corr": ["\\mathbb{C}\\text{or}\\left[#1,#2\\right]", 2], "ai": "\\alpha", "bi": "\\beta", "dx": ["\\;\\mathrm{d}#1", 1]}}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> PyApprox
          

          
            
            <img src="../../_static/pyapprox-logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intro.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../auto_tutorials/index.html">PyApprox Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">Benchmarks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">User Reference Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user_reference_guide.html">User Reference Guide</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Reference Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../modules.html">Developer Reference Guide</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">PyApprox</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Multi-level Monte Carlo</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/auto_examples/multi_fidelity/plot_multi_level_monte_carlo.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-auto-examples-multi-fidelity-plot-multi-level-monte-carlo-py"><span class="std std-ref">here</span></a>     to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="multi-level-monte-carlo">
<span id="sphx-glr-auto-examples-multi-fidelity-plot-multi-level-monte-carlo-py"></span><h1>Multi-level Monte Carlo<a class="headerlink" href="#multi-level-monte-carlo" title="Permalink to this headline">¶</a></h1>
<p>This tutorial builds upon <a class="reference internal" href="../../auto_tutorials/multi_fidelity/plot_approximate_control_variate_monte_carlo.html#sphx-glr-auto-tutorials-multi-fidelity-plot-approximate-control-variate-monte-carlo-py"><span class="std std-ref">Approximate Control Variate Monte Carlo</span></a> and describes how the pioneering work of Multi-level Monte Carlo <a class="reference internal" href="../../auto_tutorials/multi_fidelity/plot_multi_level_monte_carlo.html#cgstcvs2011" id="id1"><span>[CGSTCVS2011]</span></a>, <a class="reference internal" href="../../auto_tutorials/multi_fidelity/plot_multi_level_monte_carlo.html#gor2008" id="id2"><span>[GOR2008]</span></a> can be used to estimate the mean of a high-fidelity model using multiple low fidelity models.</p>
<div class="section" id="two-model-mlmc">
<h2>Two Model MLMC<a class="headerlink" href="#two-model-mlmc" title="Permalink to this headline">¶</a></h2>
<p>The two model MLMC estimator is very similar to the CVMC estimator and is based upon the observations that we can estimate</p>
<div class="math notranslate nohighlight">
\[\mean{f_0}=\mean{f_1}+\mean{f_0-f_1}\]</div>
<p>using the following unbiased estimator</p>
<div class="math notranslate nohighlight">
\[Q_{0,\mathcal{Z}}^\mathrm{ML}=N_1^{-1}\sum_{i=1}^{N_1} f_{1,\mathcal{Z}_1}^{(i)}+N_0^{-1}\sum_{i=1}^{N_0} \left(f_{0,\mathcal{Z}_0}^{(i)}-f_{1,\mathcal{Z}_0}^{(i)}\right)\]</div>
<p>Where samples <span class="math notranslate nohighlight">\(\mathcal{Z}=\mathcal{Z}_0\bigcup\mathcal{Z}_1\)</span> with <span class="math notranslate nohighlight">\(\mathcal{Z}_0\bigcap\mathcal{Z}_1=\emptyset\)</span> and <span class="math notranslate nohighlight">\(f_{\alpha,\mathcal{Z}_\kappa}^{(i)}\)</span> denotes an evaluation of <span class="math notranslate nohighlight">\(f_\alpha\)</span> using a sample from the set <span class="math notranslate nohighlight">\(\mathcal{Z}_\kappa\)</span>. More simply to evaluate this estimator we must evaluate the low fidelity model at a set of samples  <span class="math notranslate nohighlight">\(\mathcal{Z}_1\)</span> and then evaluate the difference between the two models at another independet set <span class="math notranslate nohighlight">\(\mathcal{Z}_0\)</span>.</p>
<p>To simplify notation let</p>
<div class="math notranslate nohighlight">
\[Y_{\alpha,\mathcal{Z}_{\alpha-1}}=-N_{\alpha}^{-1}\sum_{i=1}^{N_{\alpha}} \left(f_{\alpha+1,\mathcal{Z}_{\alpha}}^{(i)}-f_{\alpha,\mathcal{Z}_{\alpha}}^{(i)}\right)\]</div>
<p>so we can write</p>
<div class="math notranslate nohighlight">
\[Q_{0,\mathcal{Z}}^\mathrm{ML}=Y_{0,\mathcal{Z}_0}+Y_{1,\mathcal{Z}_1}\]</div>
<p>where <span class="math notranslate nohighlight">\(f_{2}=0\)</span>.
The variance of this estimator is given by</p>
<div class="math notranslate nohighlight">
\[\var{Q_{0,\mathcal{Z}}^\mathrm{ML}}=\var{Y_{0,\mathcal{Z}_0}+Y_{1,\mathcal{Z}_1}}=N_0^{-1}\var{Y_{0,\mathcal{Z}_0}}+N_1^{-1}\var{Y_{1,\mathcal{Z}_1}}\]</div>
<p>since <span class="math notranslate nohighlight">\(\mathcal{Z}_0\bigcap\mathcal{Z}_1=\emptyset.\)</span></p>
<p>From the previous equation we can see that MLMC works well if the variance of the difference between models is smaller than the variance of either model. Although the variance of the low-fidelity model is likely high, we can set <span class="math notranslate nohighlight">\(N_0\)</span> to be large because the cost of evaluating the low-fidelity model is low. If the variance of the discrepancy is smaller we only need a smaller number of samples so that the two sources of error (the two terms on the RHS of the previous equation) are balanced.</p>
<p>Note above and below we assume that <span class="math notranslate nohighlight">\(f_0\)</span> is the high-fidelity model, but typical multi-level literature assumes <span class="math notranslate nohighlight">\(f_M\)</span> is the high-fidelity model. We adopt the reverse ordering to be consistent with control variate estimator literature.</p>
</div>
<div class="section" id="many-model-mlmc">
<h2>Many Model MLMC<a class="headerlink" href="#many-model-mlmc" title="Permalink to this headline">¶</a></h2>
<p>MLMC can easily be extended to estimator based on <span class="math notranslate nohighlight">\(M+1\)</span> models. Letting <span class="math notranslate nohighlight">\(f_0,\ldots,f_M\)</span> be an ensemble of <span class="math notranslate nohighlight">\(M+1\)</span> models ordered by decreasing fidelity and cost (note typically MLMC literature reverses this order),  we simply introduce estimates of the differences between the remaining models. That is</p>
<div class="math notranslate nohighlight" id="equation-eq-ml-estimator">
<span class="eqno">(1)<a class="headerlink" href="#equation-eq-ml-estimator" title="Permalink to this equation">¶</a></span>\[Q_{0,\mathcal{Z}}^\mathrm{ML} = \sum_{\alpha=0}^M Y_{\alpha,\mathcal{Z}_\alpha}, \quad f_{M+1}=0\]</div>
<p>To compute this estimator we use the following algorithm, tarting with :math:<a href="#id3"><span class="problematic" id="id4">`</span></a>alpha=M`$</p>
<ol class="arabic simple">
<li><p>Draw  <span class="math notranslate nohighlight">\(N_\alpha\)</span> samples randomly from the PDF  <span class="math notranslate nohighlight">\(\pdf\)</span> of the random variables.</p></li>
<li><p>Estimate <span class="math notranslate nohighlight">\(f_{\alpha}\)</span> and <span class="math notranslate nohighlight">\(f_{\alpha}\)</span> at the samples <span class="math notranslate nohighlight">\(\mathcal{Z}_\alpha\)</span></p></li>
<li><p>Compute the discrepancies <span class="math notranslate nohighlight">\(Y_{\alpha,\mathcal{Z}_\alpha}\)</span> at each sample.</p></li>
<li><p>Decrease <span class="math notranslate nohighlight">\(\alpha\)</span> and repeat steps 1-3. until <span class="math notranslate nohighlight">\(\alpha=0\)</span>.</p></li>
<li><p>Compute the ML estimator using <a class="reference internal" href="../../auto_tutorials/multi_fidelity/plot_multi_level_monte_carlo.html#equation-eq-ml-estimator">(1)</a></p></li>
</ol>
<p>The sampling strategy used for MLMC is shown in figure <a class="reference internal" href="../../auto_tutorials/multi_fidelity/plot_multi_level_monte_carlo.html#mlmc-sample-allocation"><span class="std std-ref">MLMC sampling strategy</span></a>. Here it is clear that we evaluate only the lowest fidelity model with <span class="math notranslate nohighlight">\(\mathcal{Z}_M\)</span> (this follows from the assumption that <span class="math notranslate nohighlight">\(f_{M+1}=0\)</span>) and to to evaluate each discrepancy we must evaluate two consecutive models at the sets <span class="math notranslate nohighlight">\(\mathcal{Z}_\alpha\)</span>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 100%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><div class="figure align-center" id="id11">
<span id="mlmc-sample-allocation"></span><a class="reference internal image-reference" href="../../_images/mlmc.png"><img alt="../../_images/mlmc.png" src="../../_images/mlmc.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-text">MLMC sampling strategy</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
</td>
</tr>
</tbody>
</table>
<p>Because we use independent samples to estimate the expectation of each discrepancy, the variance of the estimator is</p>
<div class="math notranslate nohighlight">
\[\var{Q_{0,\mathcal{Z}}^\mathrm{ML}} = \sum_{\alpha=0}^M N_\alpha\var{Y_{\alpha,\mathcal{Z}_\alpha}}\]</div>
<p>Lets setup a problem to compute an MLMC estimate of <span class="math notranslate nohighlight">\(\mean{f_0}\)</span>
using the following ensemble of models, taken from <span id="id5">[PWGSIAM2016]</span>, which simulate a short column with a rectangular cross-sectional area subject to bending and axial force.</p>
<div class="math notranslate nohighlight">
\[\begin{split}f_0(z)&amp;=(1 - 4M/(b(h^2)Y) - (P/(bhY))^2)\\
f_1(z)&amp;=(1 - 3.8M/(b(h^2)Y) - ((P(1 + (M-2000)/4000))/(bhY))^2)\\
f_2(z)&amp;=(1 - M/(b(h^2)Y) - (P/(bhY))^2)\\
f_3(z)&amp;=(1 - M/(b(h^2)Y) - (P(1 + M)/(bhY))^2)\\
f_4(z)&amp;=(1 - M/(b(h^2)Y) - (P(1 + M)/(hY))^2)\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(z = (b,h,P,M,Y)^T\)</span></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyapprox</span> <span class="k">as</span> <span class="nn">pya</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">pyapprox.tests.test_control_variate_monte_carlo</span> <span class="kn">import</span> \
    <span class="n">TunableModelEnsemble</span><span class="p">,</span> <span class="n">ShortColumnModelEnsemble</span><span class="p">,</span> <span class="n">PolynomialModelEnsemble</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span><span class="p">,</span><span class="n">norm</span><span class="p">,</span><span class="n">lognorm</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">short_column_model</span> <span class="o">=</span> <span class="n">ShortColumnModelEnsemble</span><span class="p">()</span>
<span class="n">model_ensemble</span> <span class="o">=</span> <span class="n">pya</span><span class="o">.</span><span class="n">ModelEnsemble</span><span class="p">(</span>
    <span class="p">[</span><span class="n">short_column_model</span><span class="o">.</span><span class="n">m0</span><span class="p">,</span><span class="n">short_column_model</span><span class="o">.</span><span class="n">m1</span><span class="p">,</span><span class="n">short_column_model</span><span class="o">.</span><span class="n">m2</span><span class="p">])</span>

<span class="n">costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">target_cost</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">short_column_model</span><span class="o">.</span><span class="n">get_covariance_matrix</span><span class="p">()[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span><span class="n">idx</span><span class="p">)]</span>
<span class="c1"># generate pilot samples to estimate correlation</span>
<span class="c1"># npilot_samples = int(1e4)</span>
<span class="c1"># cov = pya.estimate_model_ensemble_covariance(</span>
<span class="c1">#    npilot_samples,short_column_model.generate_samples,model_ensemble)[0]</span>

<span class="c1"># define the sample allocation</span>
<span class="n">nhf_samples</span><span class="p">,</span><span class="n">nsample_ratios</span> <span class="o">=</span> <span class="n">pya</span><span class="o">.</span><span class="n">allocate_samples_mlmc</span><span class="p">(</span>
    <span class="n">cov</span><span class="p">,</span> <span class="n">costs</span><span class="p">,</span> <span class="n">target_cost</span><span class="p">)[:</span><span class="mi">2</span><span class="p">]</span>
<span class="c1"># generate sample sets</span>
<span class="n">samples</span><span class="p">,</span><span class="n">values</span> <span class="o">=</span><span class="n">pya</span><span class="o">.</span><span class="n">generate_samples_and_values_mlmc</span><span class="p">(</span>
    <span class="n">nhf_samples</span><span class="p">,</span><span class="n">nsample_ratios</span><span class="p">,</span><span class="n">model_ensemble</span><span class="p">,</span>
    <span class="n">short_column_model</span><span class="o">.</span><span class="n">generate_samples</span><span class="p">)</span>
<span class="c1"># compute mean using only hf data</span>
<span class="n">hf_mean</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1"># compute mlmc control variate weights</span>
<span class="n">eta</span> <span class="o">=</span> <span class="n">pya</span><span class="o">.</span><span class="n">get_mlmc_control_variate_weights</span><span class="p">(</span><span class="n">cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># compute MLMC mean</span>
<span class="n">mlmc_mean</span> <span class="o">=</span> <span class="n">pya</span><span class="o">.</span><span class="n">compute_approximate_control_variate_mean_estimate</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># get the true mean of the high-fidelity model</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="n">short_column_model</span><span class="o">.</span><span class="n">get_means</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MLMC error&#39;</span><span class="p">,</span><span class="nb">abs</span><span class="p">(</span><span class="n">mlmc_mean</span><span class="o">-</span><span class="n">true_mean</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MC error&#39;</span><span class="p">,</span><span class="nb">abs</span><span class="p">(</span><span class="n">hf_mean</span><span class="o">-</span><span class="n">true_mean</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>MLMC error 0.017085105945351176
MC error 0.019879664393831153
</pre></div>
</div>
<p>These errors are comparable. However these errors are only for one realization of the samples sets. To obtain a clearer picture on the benefits of MLMC we need to look at the variance of the estimator.</p>
<p>By viewing MLMC as a control variate we can derive its variance reduction <a class="reference internal" href="../../auto_tutorials/multi_fidelity/plot_many_model_approximate_control_variate_monte_carlo.html#ggejjcp2020" id="id6"><span>[GGEJJCP2020]</span></a></p>
<div class="math notranslate nohighlight" id="equation-mlmc-variance-reduction">
<span class="eqno">(2)<a class="headerlink" href="#equation-mlmc-variance-reduction" title="Permalink to this equation">¶</a></span>\[\gamma+1 = - \eta_1^2 \tau_{1}^2 - 2 \eta_1 \rho_{1} \tau_{1} - \eta_M^2 \frac{\tau_{M}}{\hat{r}_{M}} - \sum_{i=2}^M \frac{1}{\hat{r}_{i-1}}\left( \eta_i^2 \tau_{i}^2 + \tau_{i-1}^2 \tau_{i-1}^2 - 2 \eta_i \eta_{i-1} \rho_{i,i-1} \tau_{i} \tau_{i-1} \right),\]</div>
<p>where  <span class="math notranslate nohighlight">\(\tau_\alpha=\left(\frac{\var{Q_\alpha}}{\var{Q_0}}\right)^{\frac{1}{2}}\)</span>. Recall that and <span class="math notranslate nohighlight">\(\hat{r}_\alpha=\lvert\mathcal{Z}_{\alpha,2}\rvert/N\)</span> is the ratio of the cardinality of the sets <span class="math notranslate nohighlight">\(\mathcal{Z}_{\alpha,2}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Z}_{0,2}\)</span>.</p>
<p>The following code computes the variance reduction of the MLMC estimator, using the 2 models <span class="math notranslate nohighlight">\(f_0,f_1\)</span>. The variance reduction is estimated numerically by  running MLMC repeatedly with different realizations of the sample sets. The function <code class="docutils literal notranslate"><span class="pre">get_rsquared_mlmc</span></code> is used to return the theoretical variance reduction.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ntrials</span><span class="o">=</span><span class="mf">1e1</span>
<span class="n">get_cv_weights_mlmc</span> <span class="o">=</span> <span class="n">pya</span><span class="o">.</span><span class="n">get_mlmc_control_variate_weights_pool_wrapper</span>
<span class="n">means1</span><span class="p">,</span> <span class="n">numerical_var_reduction1</span><span class="p">,</span> <span class="n">true_var_reduction1</span> <span class="o">=</span> \
    <span class="n">pya</span><span class="o">.</span><span class="n">estimate_variance_reduction</span><span class="p">(</span>
        <span class="n">model_ensemble</span><span class="p">,</span> <span class="n">cov</span><span class="p">[:</span><span class="mi">2</span><span class="p">,:</span><span class="mi">2</span><span class="p">],</span> <span class="n">short_column_model</span><span class="o">.</span><span class="n">generate_samples</span><span class="p">,</span>
        <span class="n">pya</span><span class="o">.</span><span class="n">allocate_samples_mlmc</span><span class="p">,</span>
        <span class="n">pya</span><span class="o">.</span><span class="n">generate_samples_and_values_mlmc</span><span class="p">,</span> <span class="n">get_cv_weights_mlmc</span><span class="p">,</span>
        <span class="n">pya</span><span class="o">.</span><span class="n">get_rsquared_mlmc</span><span class="p">,</span><span class="n">ntrials</span><span class="o">=</span><span class="n">ntrials</span><span class="p">,</span><span class="n">max_eval_concurrency</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">costs</span><span class="o">=</span><span class="n">costs</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span><span class="n">target_cost</span><span class="o">=</span><span class="n">target_cost</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Theoretical 2 model MLMC variance reduction&quot;</span><span class="p">,</span><span class="n">true_var_reduction1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Achieved 2 model MLMC variance reduction&quot;</span><span class="p">,</span><span class="n">numerical_var_reduction1</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Theoretical 2 model MLMC variance reduction 0.02733258395033522
Achieved 2 model MLMC variance reduction 0.040306711980366165
</pre></div>
</div>
<p>The numerical estimate of the variance reduction is consistent with the theory.
Now let us compute the theoretical variance reduction using 3 models</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">true_var_reduction2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">pya</span><span class="o">.</span><span class="n">get_rsquared_mlmc</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span><span class="n">nsample_ratios</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Theoretical 3 model MLMC variance reduction&quot;</span><span class="p">,</span><span class="n">true_var_reduction2</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Theoretical 3 model MLMC variance reduction 0.024681101214007017
</pre></div>
</div>
<p>The variance reduction obtained using three models is only slightly better than when using two models. The difference in variance reduction is dependent on the correlations between the models and the number of samples assigned to each model.</p>
<p>MLMC works extremely well when the variance between the model discrepancies decays with increaseing fidelity. However one needs to be careful that the variance between discrepancies does indeed decay. Sometimes when the models correspond to different mesh discretizations. Increasing the mesh resolution does not always produce a smaller discrepancy. The following example shows that, for this example, adding a third model actually increases the variance of the MLMC estimator.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_ensemble</span> <span class="o">=</span> <span class="n">pya</span><span class="o">.</span><span class="n">ModelEnsemble</span><span class="p">(</span>
    <span class="p">[</span><span class="n">short_column_model</span><span class="o">.</span><span class="n">m0</span><span class="p">,</span><span class="n">short_column_model</span><span class="o">.</span><span class="n">m3</span><span class="p">,</span><span class="n">short_column_model</span><span class="o">.</span><span class="n">m4</span><span class="p">])</span>
<span class="n">idx</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">cov3</span> <span class="o">=</span> <span class="n">short_column_model</span><span class="o">.</span><span class="n">get_covariance_matrix</span><span class="p">()[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span><span class="n">idx</span><span class="p">)]</span>
<span class="n">true_var_reduction3</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">pya</span><span class="o">.</span><span class="n">get_rsquared_mlmc</span><span class="p">(</span><span class="n">cov3</span><span class="p">,</span><span class="n">nsample_ratios</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Theoretical 3 model MLMC variance reduction for a pathalogical example&quot;</span><span class="p">,</span><span class="n">true_var_reduction3</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Theoretical 3 model MLMC variance reduction for a pathalogical example 11467238870623.877
</pre></div>
</div>
<p>Using MLMC for this ensemble of models creates an estimate with a variance orders of magnitude larger than just using the high-fidelity model. When using models that do not admit a hierarchical structure, alternative approaches are needed. We will introduce such estimators in future tutorials.</p>
</div>
<div class="section" id="optimal-sample-allocation">
<h2>Optimal Sample Allocation<a class="headerlink" href="#optimal-sample-allocation" title="Permalink to this headline">¶</a></h2>
<p>Up to this point we have assumed that the number of samples allocated to each model was fixed. However one major advantage of MLMC is that we can analytically determine the optimal number of samples assigned to each model used in a MLMC estimator. This section follows closely the exposition in <a class="reference internal" href="../../auto_tutorials/multi_fidelity/plot_multi_level_monte_carlo.html#gan2015" id="id7"><span>[GAN2015]</span></a>.</p>
<p>Let <span class="math notranslate nohighlight">\(C_\alpha\)</span> be the cost of evaluating the function <span class="math notranslate nohighlight">\(f_\alpha\)</span> at a single sample, then the total cost of the MLMC estimator is</p>
<div class="math notranslate nohighlight">
\[C_{\mathrm{tot}}=\sum_{l=0}^M C_\alpha N_\alpha\]</div>
<p>Now recall that the variance of the estimator is</p>
<div class="math notranslate nohighlight">
\[\var{Q_0^\mathrm{ML}}=\sum_{\alpha=0}^M \var{Y_\alpha}N_\alpha,\]</div>
<p>where <span class="math notranslate nohighlight">\(Y_\alpha\)</span> is the disrepancy between two consecutive models, e.g. <span class="math notranslate nohighlight">\(f_{\alpha-1}-f_\alpha\)</span> and <span class="math notranslate nohighlight">\(N_\alpha\)</span> be the number of samples allocated to resolving the discrepancy, i.e. <span class="math notranslate nohighlight">\(N_\alpha=\lvert\hat{\mathcal{Z}}_\alpha\rvert\)</span>. Then For a fixed variance <span class="math notranslate nohighlight">\(\epsilon^2\)</span> the cost of the MLMC estimator can be minimized, by solving</p>
<div class="math notranslate nohighlight">
\[\begin{split}\min_{N_0,\ldots,N_M} &amp; \sum_{\alpha=0}^M\left(N_\alpha C_\alpha\right)\\
\mathrm{subject}\; \mathrm{to} &amp;\sum_{\alpha=0}^M\left(N_\alpha^{-1}\var{Y_\alpha}\right)=\epsilon^2\end{split}\]</div>
<p>or alternatively by introducing the lagrange multiplier <span class="math notranslate nohighlight">\(\lambda^2\)</span> we can minimize</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathcal{J}(N_0,\ldots,N_M,\lambda)&amp;=\sum_{\alpha=0}^M\left(N_\alpha C_\alpha\right)+\lambda^2\left(\sum_{\alpha=0}^M\left(N_\alpha^{-1}\var{Y_\alpha}\right)-\epsilon^2\right)\\
&amp;=\sum_{\alpha=0}^M\left(N_\alpha C_\alpha+\lambda^2N_\alpha^{-1}\var{Y_\alpha}\right)-\lambda^2\epsilon^2\end{split}\]</div>
<p>To find the minimum we set the gradient of this expression to zero:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial \mathcal{J}^\mathrm{ML}}{N_\alpha}&amp;=C_\alpha-\lambda^2N_\alpha^{-2}\var{Y_\alpha}=0\\
\implies C_\alpha&amp;=\lambda^2N_\alpha^{-2}\var{Y_\alpha}\\
\implies N_\alpha&amp;=\lambda\sqrt{\var{Y_\alpha}C_\alpha^{-1}}\end{split}\]</div>
<p>The constraint is satisifed by noting</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{J}}{\lambda^2}=\sum_{\alpha=0}^M N_\alpha^{-1}\var{Y_\alpha}-\epsilon^2=0\]</div>
<p>Recalling that we can write the total variance as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\var{Q_{0,\mathcal{Z}}^\mathrm{ML}}&amp;=\sum_{\alpha=0}^M N_\alpha^{-1} \var{Y_\alpha}\\
&amp;=\sum_{\alpha=0}^M \lambda^{-1}\var{Y_\alpha}^{-\frac{1}{2}}C_\alpha^{\frac{1}{2}}\var{Y_\alpha}\\
&amp;=\lambda^{-1}\sum_{\alpha=0}^M\sqrt{\var{Y_\alpha}C_\alpha}=\epsilon^2\\
\implies \lambda &amp;= \epsilon^{-2}\sum_{\alpha=0}^M\sqrt{\var{Y_\alpha}C_\alpha}\end{split}\]</div>
<p>Then substituting <span class="math notranslate nohighlight">\(\lambda\)</span> into the following</p>
<div class="math notranslate nohighlight">
\[\begin{split}N_\alpha C_\alpha&amp;=\lambda\sqrt{\var{Y_\alpha}C_\alpha^{-1}}C_\alpha\\
&amp;=\lambda\sqrt{\var{Y_\alpha}C_\alpha}\\
&amp;=\epsilon^{-2}\left(\sum_{\alpha=0}^M\sqrt{\var{Y_\alpha}C_\alpha}\right)\sqrt{\var{Y_\alpha}C_\alpha}\end{split}\]</div>
<p>allows us to determine the smallest total cost that generates and estimator with the desired variance.</p>
<div class="math notranslate nohighlight">
\[\begin{split}C_\mathrm{tot}&amp;=\sum_{\alpha=0}^M N_\alpha C_\alpha\\
&amp;=\sum_{\alpha=0}^M \epsilon^{-2}\left(\sum_{\alpha=0}^M\sqrt{\var{Y_\alpha}C_\alpha}\right)\sqrt{\var{Y_\alpha}C_\alpha}\\
&amp;=\epsilon^{-2}\left(\sum_{\alpha=0}^M\sqrt{\var{Y_\alpha}C_\alpha}\right)^2\end{split}\]</div>
<p>To demonstrate the effectiveness of MLMC consider the model ensemble</p>
<div class="math notranslate nohighlight">
\[f_\alpha(\rv)=\rv^{5-\alpha}, \quad \alpha=0,\ldots,4\]</div>
<p>where each model is the function of a single uniform random variable defined on the unit interval <span class="math notranslate nohighlight">\([0,1]\)</span>. The following code computes the variance of the MLMC estimator for different target costs using the optimal sample allocation using an exact estimate of the covariance between models and an approximation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pyapprox</span> <span class="k">as</span> <span class="nn">pya</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">pyapprox.tests.test_control_variate_monte_carlo</span> <span class="kn">import</span> \
    <span class="n">PolynomialModelEnsemble</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">poly_model</span> <span class="o">=</span> <span class="n">PolynomialModelEnsemble</span><span class="p">()</span>
<span class="n">model_ensemble</span> <span class="o">=</span> <span class="n">pya</span><span class="o">.</span><span class="n">ModelEnsemble</span><span class="p">(</span><span class="n">poly_model</span><span class="o">.</span><span class="n">models</span><span class="p">)</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">poly_model</span><span class="o">.</span><span class="n">get_covariance_matrix</span><span class="p">()</span>
<span class="n">target_costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1e1</span><span class="p">,</span><span class="mf">1e2</span><span class="p">,</span><span class="mf">1e3</span><span class="p">,</span><span class="mf">1e4</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">10</span><span class="o">**-</span><span class="n">ii</span> <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
<span class="n">model_labels</span><span class="o">=</span><span class="p">[</span><span class="sa">r</span><span class="s1">&#39;$f_0$&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;$f_1$&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;$f_2$&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;$f_3$&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;$f_4$&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">plot_mlmc_error</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define function to create plot so we can create it later and add</span>
<span class="sd">    other estimator error curves.</span>

<span class="sd">    Note this is only necessary for sphinx-gallery docs. If just using the</span>
<span class="sd">    jupyter notebooks they create then we do not need this function definition</span>
<span class="sd">    and simply need to call fig at end of next cell to regenerate plot.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">variances</span><span class="p">,</span> <span class="n">nsamples_history</span> <span class="o">=</span> <span class="p">[],[]</span>
    <span class="n">npilot_samples</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="n">pya</span><span class="o">.</span><span class="n">MLMC</span>
    <span class="k">for</span> <span class="n">target_cost</span> <span class="ow">in</span> <span class="n">target_costs</span><span class="p">:</span>
        <span class="c1"># compute variance  using exact covariance for sample allocation</span>
        <span class="n">est</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span><span class="n">costs</span><span class="p">)</span>
        <span class="n">nhf_samples</span><span class="p">,</span><span class="n">nsample_ratios</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">allocate_samples</span><span class="p">(</span><span class="n">target_cost</span><span class="p">)[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">variances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">get_variance</span><span class="p">(</span><span class="n">nhf_samples</span><span class="p">,</span><span class="n">nsample_ratios</span><span class="p">))</span>
        <span class="n">nsamples_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">get_nsamples</span><span class="p">(</span><span class="n">nhf_samples</span><span class="p">,</span><span class="n">nsample_ratios</span><span class="p">))</span>
        <span class="c1"># compute single fidelity Monte Carlo variance</span>
        <span class="n">total_cost</span> <span class="o">=</span> <span class="n">nsamples_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">costs</span><span class="p">)</span>
        <span class="n">variances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cov</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="nb">int</span><span class="p">(</span><span class="n">total_cost</span><span class="o">/</span><span class="n">costs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">nsamples_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">total_cost</span><span class="o">/</span><span class="n">costs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="c1"># compute variance using approx covariance for sample allocation</span>
        <span class="c1"># use nhf_samples from previous target_cost as npilot_samples.</span>
        <span class="c1"># This way the pilot samples are only an additional cost at the first</span>
        <span class="c1"># step. This code does not do this though for simplicity</span>
        <span class="n">cov_approx</span> <span class="o">=</span> <span class="n">pya</span><span class="o">.</span><span class="n">estimate_model_ensemble_covariance</span><span class="p">(</span>
            <span class="n">npilot_samples</span><span class="p">,</span><span class="n">poly_model</span><span class="o">.</span><span class="n">generate_samples</span><span class="p">,</span><span class="n">model_ensemble</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">est</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">(</span><span class="n">cov_approx</span><span class="p">,</span><span class="n">costs</span><span class="p">)</span>
        <span class="n">nhf_samples</span><span class="p">,</span><span class="n">nsample_ratios</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">allocate_samples</span><span class="p">(</span><span class="n">target_cost</span><span class="p">)[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">variances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">get_variance</span><span class="p">(</span><span class="n">nhf_samples</span><span class="p">,</span><span class="n">nsample_ratios</span><span class="p">))</span>
        <span class="n">nsamples_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">get_nsamples</span><span class="p">(</span><span class="n">nhf_samples</span><span class="p">,</span><span class="n">nsample_ratios</span><span class="p">))</span>
        <span class="n">npilot_samples</span> <span class="o">=</span> <span class="n">nhf_samples</span>

    <span class="n">fig</span><span class="p">,</span><span class="n">axs</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
    <span class="n">pya</span><span class="o">.</span><span class="n">plot_acv_sample_allocation</span><span class="p">(</span><span class="n">nsamples_history</span><span class="p">[::</span><span class="mi">3</span><span class="p">],</span><span class="n">costs</span><span class="p">,</span><span class="n">model_labels</span><span class="p">,</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">total_costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">nsamples_history</span><span class="p">[::</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">costs</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">total_costs</span><span class="p">,</span><span class="n">variances</span><span class="p">[::</span><span class="mi">3</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathrm</span><span class="si">{MLMC}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">mc_line</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">total_costs</span><span class="p">,</span><span class="n">variances</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">3</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathrm</span><span class="si">{MC}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">total_costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">nsamples_history</span><span class="p">[</span><span class="mi">2</span><span class="p">::</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">costs</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">total_costs</span><span class="p">,</span><span class="n">variances</span><span class="p">[</span><span class="mi">2</span><span class="p">::</span><span class="mi">3</span><span class="p">],</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
                  <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathrm{MLMC^\dagger}$&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathrm</span><span class="si">{Total}</span><span class="s1">\;\mathrm</span><span class="si">{Cost}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathrm</span><span class="si">{Variance}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span><span class="n">axs</span>

<span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plot_mlmc_error</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="plot multi level monte carlo" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_multi_level_monte_carlo_001.png" />
<p>The left plot shows that the variance of the MLMC estimator is over and order of magnitude smaller than the variance of the single fidelity MC estimator for a fixed cost. The impact of using the approximate covariance is more significant for small samples sizes.</p>
<p>The right plot depicts the percentage of the computational cost due to evaluating each model. The numbers in the bars represent the number of samples allocated to each model. Relative to the low fidelity models only a small number of samples are allocated to the high-fidelity model, however evaluating these samples represents approximately 50% of the total cost.</p>
</div>
<div class="section" id="multi-index-monte-carlo">
<h2>Multi-index Monte Carlo<a class="headerlink" href="#multi-index-monte-carlo" title="Permalink to this headline">¶</a></h2>
<p>Multi-Level Monte Carlo utilizes a sequence of models controlled by a single hyper-parameter, specifying the level of discretization for example, in a manner that balances computational cost with increasing accuracy. In many applications, however, multiple hyper-parameters may control the model discretization, such as the mesh and time step sizes. In these situations, it may not be clear how to construct a one-dimensional hierarchy represented by a scalar hyper-parameter. To overcome this limitation, a generalization of multi-level Monte Carlo, referred to as multi-index stochastic collocation (MIMC), was developed to deal with multivariate hierarchies with multiple refinement hyper-parameters <a class="reference internal" href="../../auto_tutorials/multi_fidelity/plot_multi_level_monte_carlo.html#hntnm2016" id="id8"><span>[HNTNM2016]</span></a>.  PyApprox does not implement MIMC but a surrogate based version called Multi-index stochastic collocation (MISC) is presented in this tutorial <a class="reference internal" href="../../auto_tutorials/multi_fidelity/plot_multi_index_collocation.html#sphx-glr-auto-tutorials-multi-fidelity-plot-multi-index-collocation-py"><span class="std std-ref">Multi-index Stochastic Collocation</span></a>.</p>
<div class="section" id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h3>
<dl class="citation">
<dt class="label" id="cgstcvs2011"><span class="brackets"><a class="fn-backref" href="#id1">CGSTCVS2011</a></span></dt>
<dd><p><a href="#id9"><span class="problematic" id="id10">`</span></a>K.A. Cliffe, M.B. Giles, R. Scheichl, A.L. Teckentrup, Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients, Comput. Vis. Sci., 14, 3-15, 2011. &lt;<a class="reference external" href="https://doi.org/10.1007/s00791-011-0160-x">https://doi.org/10.1007/s00791-011-0160-x</a>&gt;`_0</p>
</dd>
<dt class="label" id="gor2008"><span class="brackets"><a class="fn-backref" href="#id2">GOR2008</a></span></dt>
<dd><p><a class="reference external" href="https://doi.org/10.1287/opre.1070.0496">M.B. Giles, Multilevel Monte Carlo path simulation, Oper. Res., 56(3), 607-617, 2008.</a></p>
</dd>
<dt class="label" id="gan2015"><span class="brackets"><a class="fn-backref" href="#id7">GAN2015</a></span></dt>
<dd><p><a class="reference external" href="https://doi.org/10.1017/S096249291500001X">M. Giles, Multilevel Monte Carlo methods, Acta Numerica, 24, 259-328, 2015.</a></p>
</dd>
<dt class="label" id="hntnm2016"><span class="brackets"><a class="fn-backref" href="#id8">HNTNM2016</a></span></dt>
<dd><p><a class="reference external" href="https://doi.org/10.1007/s00211-015-0734-5">A. Haji-Ali, F. Nobile and R. Tempone. Multi-index Monte Carlo: when sparsity meets sampling. Numerische Mathematik, 132(4), 767-806, 2016.</a></p>
</dd>
</dl>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  1.457 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-multi-fidelity-plot-multi-level-monte-carlo-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/f20fb3707aab2be74ab78fdfa78e50cd/plot_multi_level_monte_carlo.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_multi_level_monte_carlo.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/7e0b79e86df6d298042a1290323b4ec2/plot_multi_level_monte_carlo.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_multi_level_monte_carlo.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2019 National Technology &amp; Engineering Solutions of Sandia, LLC (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S. Government retains certain rights in this software.

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>