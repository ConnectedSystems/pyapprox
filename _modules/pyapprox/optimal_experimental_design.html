

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>pyapprox.optimal_experimental_design &mdash; PyApprox 1.0.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-dataframe.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"V": ["{\\boldsymbol{#1}}", 1], "mean": ["{\\mathbb{E}\\left[#1\\right]}", 1], "var": ["{\\mathbb{V}\\left[#1\\right]}", 1], "argmin": "{\\mathrm{argmin}}", "rv": "z", "reals": "\\mathbb{R}", "pdf": "\\rho", "rvdom": "\\Gamma", "coloneqq": "\\colon=", "norm": ["{\\lVert #1 \\rVert}", 1], "argmax": ["\\operatorname{argmax}"], "covar": ["\\mathbb{C}\\text{ov}\\left[#1,#2\\right]", 2], "corr": ["\\mathbb{C}\\text{or}\\left[#1,#2\\right]", 2], "ai": "\\alpha", "bi": "\\beta", "dx": ["\\;\\mathrm{d}#1", 1]}}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> PyApprox
          

          
            
            <img src="../../_static/pyapprox-logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intro.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../auto_tutorials/index.html">PyApprox Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">Benchmarks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">User Reference Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user_reference_guide.html">User Reference Guide</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Reference Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../modules.html">Developer Reference Guide</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">PyApprox</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>pyapprox.optimal_experimental_design</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for pyapprox.optimal_experimental_design</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">solve_triangular</span>
<span class="kn">import</span> <span class="nn">copy</span>

<div class="viewcode-block" id="compute_prediction_variance"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.compute_prediction_variance.html#pyapprox.optimal_experimental_design.compute_prediction_variance">[docs]</a><span class="k">def</span> <span class="nf">compute_prediction_variance</span><span class="p">(</span><span class="n">design_prob_measure</span><span class="p">,</span><span class="n">pred_factors</span><span class="p">,</span>
                                <span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">regression_type</span><span class="o">=</span><span class="s1">&#39;lstsq&#39;</span><span class="p">):</span>
    <span class="n">M0</span><span class="p">,</span><span class="n">M1</span><span class="o">=</span><span class="n">get_M0_and_M1_matrices</span><span class="p">(</span>
        <span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_prob_measure</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="n">regression_type</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">M1</span><span class="p">,</span><span class="n">pred_factors</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">M0</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">M0u</span> <span class="o">=</span> <span class="n">M0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
        <span class="n">variances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span><span class="o">*</span><span class="n">M0u</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">variances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pred_factors</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">u</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">variances</span></div>

<div class="viewcode-block" id="compute_homoscedastic_outer_products"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.compute_homoscedastic_outer_products.html#pyapprox.optimal_experimental_design.compute_homoscedastic_outer_products">[docs]</a><span class="k">def</span> <span class="nf">compute_homoscedastic_outer_products</span><span class="p">(</span><span class="n">factors</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute </span>

<span class="sd">    .. math:: f(x_i)f(x_i)^T\quad \forall i=0,\ldots,M</span>

<span class="sd">    at a set of design pts :math:`x_i`.</span>
<span class="sd">    </span>
<span class="sd">    for the linear model</span>

<span class="sd">    .. math::  y(x) = F(x)\theta+\eta(x)\epsilon</span>

<span class="sd">    Parameters</span>
<span class="sd">    ---------</span>
<span class="sd">    factors : np.ndarray (M,N)</span>
<span class="sd">        The N factors F of the linear model evaluated at the M design pts</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    homoscedastic_outer_products : np.ndarray (N,N,M)</span>
<span class="sd">       The outer products of each row of F with itself, i.e. </span>
<span class="sd">       :math:`f(x_i)f(x_i)^T`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_design_pts</span><span class="p">,</span><span class="n">num_factors</span> <span class="o">=</span> <span class="n">factors</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">homoscedastic_outer_products</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
        <span class="p">(</span><span class="n">num_factors</span><span class="p">,</span><span class="n">num_factors</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">):</span>
        <span class="n">homoscedastic_outer_products</span><span class="p">[:,:,</span><span class="n">ii</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span>
            <span class="n">factors</span><span class="p">[</span><span class="n">ii</span><span class="p">,:],</span><span class="n">factors</span><span class="p">[</span><span class="n">ii</span><span class="p">,:])</span>
    <span class="k">return</span> <span class="n">homoscedastic_outer_products</span></div>

<div class="viewcode-block" id="get_M0_and_M1_matrices"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.get_M0_and_M1_matrices.html#pyapprox.optimal_experimental_design.get_M0_and_M1_matrices">[docs]</a><span class="k">def</span> <span class="nf">get_M0_and_M1_matrices</span><span class="p">(</span>
        <span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_prob_measure</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="n">regression_type</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the matrices :math:`M_0` and :math:`M_1` used to compute the</span>
<span class="sd">    asymptotic covariance matrix :math:`C(\mu) = M_1^{-1} M_0 M^{-1}` of the</span>
<span class="sd">    linear model</span>

<span class="sd">    .. math::  y(x) = F(x)\theta+\eta(x)\epsilon.</span>

<span class="sd">    For least squares</span>

<span class="sd">    .. math:: M_0 = \sum_{i=1}^M\eta(x_i)^2f(x_i)f(x_i)^Tr_i</span>

<span class="sd">    .. math:: M_1 = \sum_{i=1}^Mf(x_i)f(x_i)^Tr_i</span>
<span class="sd">    </span>
<span class="sd">    and for quantile regression</span>

<span class="sd">    .. math:: M_0 = \sum_{i=1}^M\frac{1}{\eta(x_i)}f(x_i)f(x_i)^Tr_i</span>

<span class="sd">    .. math:: M_1 = \sum_{i=1}^Mf(x_i)f(x_i)^Tr_i</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    homog_outer_prods : np.ndarray(num_factors,num_factors,num_design_pts)</span>
<span class="sd">        The outer products :math:`f(x_i)f(x_i)^T` for each design point </span>
<span class="sd">        :math:`x_i`</span>

<span class="sd">    design_prob_measure : np.ndarray (num_design_pts)</span>
<span class="sd">        The weights :math:`r_i` for each design point</span>

<span class="sd">    noise_multiplier : np.ndarray (num_design_pts)</span>
<span class="sd">        The design dependent noise function :math:`\eta(x)`</span>

<span class="sd">    regression_type : string</span>
<span class="sd">        The method used to compute the coefficients of the linear model. </span>
<span class="sd">        Currently supported options are ``lstsq`` and ``quantile``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    M0 : np.ndarray (num_factors,num_factors)</span>
<span class="sd">        The matrix :math:`M_0`</span>

<span class="sd">    M1 : np.ndarray (num_factors,num_factors)</span>
<span class="sd">        The matrix :math:`M_1`</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">noise_multiplier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="n">homog_outer_prods</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">design_prob_measure</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;lstsq&#39;</span><span class="p">:</span>
        <span class="n">M0</span> <span class="o">=</span> <span class="n">homog_outer_prods</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">design_prob_measure</span><span class="o">*</span><span class="n">noise_multiplier</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">M1</span> <span class="o">=</span> <span class="n">homog_outer_prods</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">design_prob_measure</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
        <span class="n">M0</span> <span class="o">=</span> <span class="n">homog_outer_prods</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">design_prob_measure</span><span class="p">)</span>
        <span class="n">M1</span> <span class="o">=</span> <span class="n">homog_outer_prods</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">design_prob_measure</span><span class="o">/</span><span class="n">noise_multiplier</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;regression type </span><span class="si">{</span><span class="n">regression_type</span><span class="si">}</span><span class="s1"> not supported&#39;</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">M0</span><span class="p">,</span><span class="n">M1</span></div>

<div class="viewcode-block" id="ioptimality_criterion"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.ioptimality_criterion.html#pyapprox.optimal_experimental_design.ioptimality_criterion">[docs]</a><span class="k">def</span> <span class="nf">ioptimality_criterion</span><span class="p">(</span><span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span>
                          <span class="n">pred_factors</span><span class="p">,</span><span class="n">design_prob_measure</span><span class="p">,</span><span class="n">return_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">noise_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">regression_type</span><span class="o">=</span><span class="s1">&#39;lstsq&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate the I-optimality criterion for a given design probability measure</span>
<span class="sd">    for the linear model</span>

<span class="sd">    .. math::  y(x) = F(x)\theta+\eta(x)\epsilon.</span>

<span class="sd">    The criteria is</span>

<span class="sd">    .. math::  \int_\Xi g(\xi) C(\mu) g(\xi) d\nu(\xi)</span>

<span class="sd">    where</span>

<span class="sd">    .. math:: C(\mu) = M_1^{-1} M_0 M^{-1}</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    homog_outer_prods : np.ndarray (num_design_factors,num_design_factors,</span>
<span class="sd">                                    num_design_pts)</span>
<span class="sd">       The outer_products :math:`f(x_i)f(x_i)^T` for each design point </span>
<span class="sd">       :math:`x_i`</span>

<span class="sd">    design_factors : np.ndarray (num_design_pts,num_design_factors)</span>
<span class="sd">       The design factors evaluated at each of the design points</span>

<span class="sd">    pred_factors : np.ndarray (num_pred_pts,num_pred_factors)</span>
<span class="sd">       The prediction factors :math:`g` evaluated at each of the prediction </span>
<span class="sd">       points</span>

<span class="sd">    design_prob_measure : np.ndarray (num_design_pts)</span>
<span class="sd">       The prob measure :math:`\mu` on the design points</span>

<span class="sd">    return_grad : boolean</span>
<span class="sd">       True  - return the value and gradient of the criterion</span>
<span class="sd">       False - return only the value of the criterion</span>

<span class="sd">    noise_multiplier : np.ndarray (num_design_pts)</span>
<span class="sd">        The design dependent noise function :math:`\eta(x)`</span>

<span class="sd">    regression_type : string</span>
<span class="sd">        The method used to compute the coefficients of the linear model. </span>
<span class="sd">        Currently supported options are ``lstsq`` and ``quantile``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    value : float</span>
<span class="sd">        The value of the objective function</span>

<span class="sd">    grad : np.ndarray (num_design_pts)</span>
<span class="sd">        The gradient of the objective function. Only if return_grad is True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">#import time</span>
    <span class="c1">#t0=time.time()</span>
    <span class="n">num_design_pts</span><span class="p">,</span> <span class="n">num_design_factors</span> <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">num_pred_pts</span><span class="p">,</span>   <span class="n">num_pred_factors</span>   <span class="o">=</span> <span class="n">pred_factors</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">design_prob_measure</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">design_prob_measure</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span>
        <span class="n">design_prob_measure</span> <span class="o">=</span> <span class="n">design_prob_measure</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">M0</span><span class="p">,</span><span class="n">M1</span><span class="o">=</span><span class="n">get_M0_and_M1_matrices</span><span class="p">(</span>
        <span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_prob_measure</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="n">regression_type</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">noise_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">Q</span><span class="p">,</span><span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">M1</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">pred_factors</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
        <span class="n">M0u</span> <span class="o">=</span> <span class="n">M0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span><span class="o">*</span><span class="n">M0u</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_pred_pts</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">return_grad</span><span class="p">):</span>
            <span class="n">gamma</span>   <span class="o">=</span> <span class="o">-</span><span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="p">,(</span><span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">M0u</span><span class="p">)))</span>
            <span class="n">Fu</span>  <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
            <span class="n">t</span>   <span class="o">=</span> <span class="n">noise_multiplier</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">Fu</span>
            <span class="n">Fgamma</span>  <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;lstsq&#39;</span><span class="p">:</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Fu</span><span class="o">*</span><span class="n">Fgamma</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">t</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Fu</span><span class="o">*</span><span class="n">Fgamma</span><span class="o">/</span><span class="n">noise_multiplier</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> \
                    <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Fu</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">gradient</span> <span class="o">/=</span> <span class="n">num_pred_pts</span>
            <span class="c1">#print(&#39;that took&#39;,time.time()-t0)</span>
            <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">gradient</span><span class="o">.</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">value</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1">#import time</span>
        <span class="c1">#t0=time.time()</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">M1</span><span class="p">,</span><span class="n">pred_factors</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="c1"># Value</span>
        <span class="c1"># We want to sum the variances, i.e. the enties of the diagonal of</span>
        <span class="c1"># pred_factors.dot(M1.dot(pred_factors.T))</span>
        <span class="c1"># We know that diag(A.T.dot(B)) = (A*B).axis=0)</span>
        <span class="c1"># The following sums over all entries of A*B we get the mean of the</span>
        <span class="c1"># variance</span>
        <span class="n">value</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pred_factors</span><span class="o">*</span><span class="n">u</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_pred_pts</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">return_grad</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">value</span>
        <span class="c1"># Gradient</span>
        <span class="n">F_M1_inv_P</span> <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">);</span>
        <span class="n">gradient</span>   <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">F_M1_inv_P</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_pred_pts</span><span class="p">;</span>
        <span class="c1">#print(&#39;That took&#39;, time.time()-t0)</span>
        <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">gradient</span><span class="o">.</span><span class="n">T</span></div>


<div class="viewcode-block" id="coptimality_criterion"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.coptimality_criterion.html#pyapprox.optimal_experimental_design.coptimality_criterion">[docs]</a><span class="k">def</span> <span class="nf">coptimality_criterion</span><span class="p">(</span><span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span>
                          <span class="n">design_prob_measure</span><span class="p">,</span><span class="n">return_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">noise_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">regression_type</span><span class="o">=</span><span class="s1">&#39;lstsq&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate the C-optimality criterion for a given design probability measure </span>
<span class="sd">    for the linear model</span>

<span class="sd">    .. math::  y(x) = F(x)\theta+\eta(x)\epsilon.</span>

<span class="sd">    The criteria is</span>

<span class="sd">    .. math:: c^T C(\mu) c </span>

<span class="sd">    where</span>

<span class="sd">    .. math:: C(\mu) = M_1^{-1} M_0 M^{-1}</span>

<span class="sd">    for some vector :math:`c`. Here we assume without loss of genearlity </span>
<span class="sd">    :math:`c=(1,1,...,1)^T`</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    homog_outer_prods : np.ndarray (num_design_factors,num_design_factors,</span>
<span class="sd">                                    num_design_pts)</span>
<span class="sd">       The hessian M_1 of the error for each design point</span>

<span class="sd">    design_factors : np.ndarray (num_design_pts,num_design_factors)</span>
<span class="sd">       The design factors evaluated at each of the design points</span>

<span class="sd">    design_prob_measure : np.ndarray (num_design_pts)</span>
<span class="sd">       The prob measure :math:`\mu` on the design points</span>

<span class="sd">    return_grad : boolean</span>
<span class="sd">       True  - return the value and gradient of the criterion</span>
<span class="sd">       False - return only the value of the criterion</span>

<span class="sd">    noise_multiplier : np.ndarray (num_design_pts)</span>
<span class="sd">        The design dependent noise function :math:`\eta(x)`</span>

<span class="sd">    regression_type : string</span>
<span class="sd">        The method used to compute the coefficients of the linear model. </span>
<span class="sd">        Currently supported options are ``lstsq`` and ``quantile``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    value : float</span>
<span class="sd">        The value of the objective function</span>

<span class="sd">    grad : np.ndarray (num_design_pts)</span>
<span class="sd">        The gradient of the objective function. Only if return_grad is True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_design_pts</span><span class="p">,</span> <span class="n">num_design_factors</span> <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_design_factors</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">design_prob_measure</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">design_prob_measure</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span>
        <span class="n">design_prob_measure</span> <span class="o">=</span> <span class="n">design_prob_measure</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">M0</span><span class="p">,</span><span class="n">M1</span><span class="o">=</span><span class="n">get_M0_and_M1_matrices</span><span class="p">(</span>
        <span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_prob_measure</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="n">regression_type</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">noise_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">Q</span><span class="p">,</span><span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">M1</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
        <span class="n">M0u</span> <span class="o">=</span> <span class="n">M0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">M0u</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">return_grad</span><span class="p">):</span>
            <span class="n">gamma</span>   <span class="o">=</span> <span class="o">-</span><span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="p">,(</span><span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">M0u</span><span class="p">)))</span>
            <span class="n">Fu</span>  <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
            <span class="n">t</span>   <span class="o">=</span> <span class="n">noise_multiplier</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">Fu</span>
            <span class="n">Fgamma</span>  <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;lstsq&#39;</span><span class="p">:</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">Fu</span><span class="o">*</span><span class="n">Fgamma</span> <span class="o">+</span> <span class="n">t</span><span class="o">**</span><span class="mi">2</span>
            <span class="k">elif</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">Fu</span><span class="o">*</span><span class="n">Fgamma</span><span class="o">/</span><span class="n">noise_multiplier</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="n">Fu</span><span class="o">**</span><span class="mi">2</span>
            <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">gradient</span><span class="o">.</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">value</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">M1</span><span class="p">,</span><span class="n">c</span><span class="p">)</span>
        <span class="n">value</span>    <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">return_grad</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">value</span>
        <span class="c1"># Gradient</span>
        <span class="n">F_M1_inv_c</span> <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">);</span>
        <span class="n">gradient</span>   <span class="o">=</span> <span class="o">-</span><span class="n">F_M1_inv_c</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">gradient</span><span class="o">.</span><span class="n">T</span></div>
        
<div class="viewcode-block" id="doptimality_criterion"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.doptimality_criterion.html#pyapprox.optimal_experimental_design.doptimality_criterion">[docs]</a><span class="k">def</span> <span class="nf">doptimality_criterion</span><span class="p">(</span><span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span>
                          <span class="n">design_prob_measure</span><span class="p">,</span><span class="n">return_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">noise_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">regression_type</span><span class="o">=</span><span class="s1">&#39;lstsq&#39;</span><span class="p">,</span>
                          <span class="n">use_cholesky</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">return_hessian</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate the D-optimality criterion for a given design probability measure</span>
<span class="sd">    for the linear model</span>

<span class="sd">    .. math::  y(x) = F(x)\theta+\eta(x)\epsilon.</span>

<span class="sd">    The criteria is</span>
<span class="sd">    </span>
<span class="sd">    .. math:: \log \mathrm{determinant} [ C(\mu) ]</span>

<span class="sd">    where</span>

<span class="sd">    .. math:: C(\mu) = M_1^{-1} M_0 M^{-1}</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    homog_outer_prods : np.ndarray (num_design_factors,num_design_factors,</span>
<span class="sd">                                    num_design_pts)</span>
<span class="sd">       The outer_products :math:`f(x_i)f(x_i)^T` for each design point </span>
<span class="sd">       :math:`x_i`</span>

<span class="sd">    design_factors : np.ndarray (num_design_pts,num_design_factors)</span>
<span class="sd">       The design factors evaluated at each of the design points</span>

<span class="sd">    design_prob_measure : np.ndarray (num_design_pts)</span>
<span class="sd">       The prob measure :math:`\mu` on the design points</span>

<span class="sd">    return_grad : boolean</span>
<span class="sd">       True  - return the value and gradient of the criterion</span>
<span class="sd">       False - return only the value of the criterion</span>

<span class="sd">    noise_multiplier : np.ndarray (num_design_pts)</span>
<span class="sd">        The design dependent noise function :math:`\eta(x)`</span>

<span class="sd">    regression_type : string</span>
<span class="sd">        The method used to compute the coefficients of the linear model. </span>
<span class="sd">        Currently supported options are ``lstsq`` and ``quantile``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    value : float</span>
<span class="sd">        The value of the objective function</span>

<span class="sd">    grad : np.ndarray (num_design_pts)</span>
<span class="sd">        The gradient of the objective function. Only if return_grad is True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">return_hessian</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">return_grad</span>
    <span class="n">num_design_pts</span><span class="p">,</span> <span class="n">num_design_factors</span> <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">design_prob_measure</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">design_prob_measure</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span>
        <span class="n">design_prob_measure</span> <span class="o">=</span> <span class="n">design_prob_measure</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1">#M1 = homog_outer_prods.dot(design_prob_measure)</span>
    <span class="n">M0</span><span class="p">,</span><span class="n">M1</span><span class="o">=</span><span class="n">get_M0_and_M1_matrices</span><span class="p">(</span>
        <span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_prob_measure</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="n">regression_type</span><span class="p">)</span>
    <span class="n">M1_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">M1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">noise_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">M0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">M1_inv</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">M1_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">gamma</span><span class="p">)))</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">return_grad</span><span class="p">):</span>
            <span class="n">M0_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">M0</span><span class="p">)</span>
            <span class="c1"># ident = np.eye(gamma.shape[0])</span>
            <span class="c1"># kappa  = M1.dot(M0_inv)</span>
            <span class="c1"># gradient = np.zeros(num_design_pts)</span>
            <span class="c1"># for ii in range(num_design_pts):</span>
            <span class="c1">#     if regression_type==&#39;lstsq&#39;:</span>
            <span class="c1">#         gradient[ii] = np.sum(kappa.dot(homog_outer_prods[:,:,ii])*(</span>
            <span class="c1">#             -2*gamma.T+noise_multiplier[ii]**2*ident).dot(M1_inv))</span>
            <span class="c1">#     elif regression_type==&#39;quantile&#39;:</span>
            <span class="c1">#         gradient[ii] = np.sum(kappa.dot(homog_outer_prods[:,:,ii])*(</span>
            <span class="c1">#             -2/noise_multiplier[:,np.newaxis][ii]*gamma.T+ident).dot(M1_inv))</span>
            <span class="c1">#return value, gradient</span>
            
            <span class="k">if</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;lstsq&#39;</span><span class="p">:</span>
                <span class="c1">#computing diagonal elements with trace is more efficient than</span>
                <span class="c1"># extracting diagonal (below) and looping through each element</span>
                <span class="c1"># (above)</span>
                <span class="c1">#gradient = -2*np.diag(design_factors.dot(M1_inv.dot(design_factors.T)))+np.diag(noise_multiplier[:,np.newaxis]*design_factors.dot(M0_inv.dot((noise_multiplier[:,np.newaxis]*design_factors).T)))</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">design_factors</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="p">(</span><span class="n">M1_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">design_factors</span><span class="o">.</span><span class="n">T</span><span class="p">)),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">noise_multiplier</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span><span class="o">*</span><span class="n">design_factors</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="p">(</span><span class="n">M0_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">((</span><span class="n">noise_multiplier</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span><span class="o">*</span><span class="n">design_factors</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">design_factors</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="p">(</span><span class="n">M1_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">((</span><span class="n">design_factors</span><span class="o">/</span><span class="n">noise_multiplier</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">design_factors</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="p">(</span><span class="n">M0_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">design_factors</span><span class="o">.</span><span class="n">T</span><span class="p">)),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">gradient</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">value</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">use_cholesky</span><span class="p">:</span>
            <span class="n">chol_factor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">M1</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">chol_factor</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">value</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">M1_inv</span><span class="p">))</span>
        <span class="c1"># Gradient</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">return_grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">use_cholesky</span><span class="p">:</span>
                <span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">solve_triangular</span>
                <span class="n">temp</span> <span class="o">=</span> <span class="n">solve_triangular</span><span class="p">(</span>
                    <span class="n">chol_factor</span><span class="p">,</span><span class="n">design_factors</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">temp</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">temp</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span><span class="c1">#precompute for hessian</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">temp</span> <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">M1_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">design_factors</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">return_hessian</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">gradient</span><span class="o">.</span><span class="n">T</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">hessian</span> <span class="o">=</span> <span class="n">temp</span><span class="o">**</span><span class="mi">2</span>
                <span class="k">return</span> <span class="n">value</span><span class="p">,</span><span class="n">gradient</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">hessian</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">value</span></div>


<div class="viewcode-block" id="aoptimality_criterion"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.aoptimality_criterion.html#pyapprox.optimal_experimental_design.aoptimality_criterion">[docs]</a><span class="k">def</span> <span class="nf">aoptimality_criterion</span><span class="p">(</span><span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span>
                          <span class="n">design_prob_measure</span><span class="p">,</span><span class="n">return_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">noise_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">regression_type</span><span class="o">=</span><span class="s1">&#39;lstsq&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate the A-optimality criterion for a given design probability measure </span>
<span class="sd">    for the linear model</span>

<span class="sd">    .. math::  y(x) = F(x)\theta+\eta(x)\epsilon.</span>

<span class="sd">    The criteria is</span>

<span class="sd">    .. math:: \mathrm{trace}[ C(\mu) ]</span>

<span class="sd">    where</span>

<span class="sd">    .. math:: C(\mu) = M_1^{-1} M_0 M^{-1}</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    homog_outer_prods : np.ndarray (num_design_factors,num_design_factors,</span>
<span class="sd">                                    num_design_pts)</span>
<span class="sd">       The hessian M_1 of the error for each design point</span>

<span class="sd">    design_factors : np.ndarray (num_design_pts,num_design_factors)</span>
<span class="sd">       The design factors evaluated at each of the design points</span>

<span class="sd">    design_prob_measure : np.ndarray (num_design_pts)</span>
<span class="sd">       The prob measure :math:`\mu` on the design points</span>

<span class="sd">    return_grad : boolean</span>
<span class="sd">       True  - return the value and gradient of the criterion</span>
<span class="sd">       False - return only the value of the criterion</span>

<span class="sd">    noise_multiplier : np.ndarray (num_design_pts)</span>
<span class="sd">        The design dependent noise function :math:`\eta(x)`</span>

<span class="sd">    regression_type : string</span>
<span class="sd">        The method used to compute the coefficients of the linear model. </span>
<span class="sd">        Currently supported options are ``lstsq`` and ``quantile``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    value : float</span>
<span class="sd">        The value of the objective function</span>

<span class="sd">    grad : np.ndarray (num_design_pts)</span>
<span class="sd">        The gradient of the objective function. Only if return_grad is True.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">num_design_pts</span><span class="p">,</span> <span class="n">num_design_factors</span> <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># [:,:,0] just changes shape from (N,N,1) to (N,N)</span>
    <span class="k">if</span> <span class="n">design_prob_measure</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">design_prob_measure</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span>
        <span class="n">design_prob_measure</span> <span class="o">=</span> <span class="n">design_prob_measure</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">M0</span><span class="p">,</span><span class="n">M1</span><span class="o">=</span><span class="n">get_M0_and_M1_matrices</span><span class="p">(</span>
        <span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_prob_measure</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="n">regression_type</span><span class="p">)</span>
    <span class="n">M1_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">M1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">noise_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">M0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">M1_inv</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">M1_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">gamma</span><span class="p">))</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">return_grad</span><span class="p">):</span>
            <span class="n">ident</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">gamma</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">M0_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">M0</span><span class="p">)</span>
            <span class="n">kappa</span>  <span class="o">=</span> <span class="n">M1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">M0_inv</span><span class="p">)</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;lstsq&#39;</span><span class="p">:</span>
                    <span class="n">gradient</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span>
                        <span class="n">M1_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">homog_outer_prods</span><span class="p">[:,:,</span><span class="n">ii</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
                        <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">gamma</span><span class="o">.</span><span class="n">T</span><span class="o">+</span><span class="n">noise_multiplier</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="o">*</span><span class="n">ident</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">M1_inv</span><span class="p">))</span>
                <span class="k">elif</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
                    <span class="n">gradient</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span>
                        <span class="n">M1_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">homog_outer_prods</span><span class="p">[:,:,</span><span class="n">ii</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
                        <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">gamma</span><span class="o">.</span><span class="n">T</span><span class="o">/</span><span class="n">noise_multiplier</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">][</span><span class="n">ii</span><span class="p">]</span><span class="o">+</span><span class="n">ident</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">M1_inv</span><span class="p">))</span>            
            <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">gradient</span><span class="o">.</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">value</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">value</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">M1_inv</span><span class="p">)</span>
        <span class="c1"># Gradient</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">return_grad</span><span class="p">):</span>
            <span class="c1">#gradient = -np.array([(M1_inv*homog_outer_prods[:,:,ii].dot(M1_inv)).sum() for ii in range(homog_outer_prods.shape[2])])</span>
            <span class="c1">#below is faster than above</span>
            <span class="n">temp</span> <span class="o">=</span> <span class="n">M1_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">M1_inv</span><span class="p">)</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">design_factors</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">design_factors</span><span class="o">.</span><span class="n">T</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">gradient</span><span class="o">.</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">value</span></div>

<div class="viewcode-block" id="roptimality_criterion"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.roptimality_criterion.html#pyapprox.optimal_experimental_design.roptimality_criterion">[docs]</a><span class="k">def</span> <span class="nf">roptimality_criterion</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span><span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span>
                          <span class="n">pred_factors</span><span class="p">,</span><span class="n">design_prob_measure</span><span class="p">,</span><span class="n">return_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">noise_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">regression_type</span><span class="o">=</span><span class="s1">&#39;lstsq&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate the R-optimality criterion for a given design probability measure </span>
<span class="sd">    for the linear model</span>

<span class="sd">    .. math::  y(x) = F(x)\theta+\eta(x)\epsilon.</span>

<span class="sd">    The criteria is</span>

<span class="sd">    .. math:: \mathrm{CVaR}\left[\sigma^2f(x)^T\left(F(\mathcal{X})^TF(\mathcal{X})\right)^{-1}f(x)\right]</span>

<span class="sd">    where</span>

<span class="sd">    .. math:: C(\mu) = M_1^{-1} M_0 M^{-1}</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    beta : float</span>
<span class="sd">       The confidence level of CVAR </span>

<span class="sd">    homog_outer_prods : np.ndarray (num_design_factors,num_design_factors,</span>
<span class="sd">                                    num_design_pts)</span>
<span class="sd">       The hessian M_1 of the error for each design point</span>

<span class="sd">    design_factors : np.ndarray (num_design_pts,num_design_factors)</span>
<span class="sd">       The design factors evaluated at each of the design points</span>

<span class="sd">    pred_factors : np.ndarray (num_pred_pts,num_pred_factors)</span>
<span class="sd">       The prediction factors :math:`g` evaluated at each of the prediction </span>
<span class="sd">       points</span>

<span class="sd">    design_prob_measure : np.ndarray (num_design_pts)</span>
<span class="sd">       The prob measure :math:`\mu` on the design points</span>

<span class="sd">    return_grad : boolean</span>
<span class="sd">       True  - return the value and gradient of the criterion</span>
<span class="sd">       False - return only the value of the criterion</span>

<span class="sd">    noise_multiplier : np.ndarray (num_design_pts)</span>
<span class="sd">        The design dependent noise function :math:`\eta(x)`</span>

<span class="sd">    regression_type : string</span>
<span class="sd">        The method used to compute the coefficients of the linear model. </span>
<span class="sd">        Currently supported options are ``lstsq`` and ``quantile``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    value : float</span>
<span class="sd">        The value of the objective function</span>

<span class="sd">    grad : np.ndarray (num_design_pts)</span>
<span class="sd">        The gradient of the objective function. Only if return_grad is True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">beta</span><span class="o">&gt;=</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">beta</span><span class="o">&lt;=</span><span class="mi">1</span>
    <span class="kn">from</span> <span class="nn">pyapprox.cvar_regression</span> <span class="kn">import</span> <span class="n">conditional_value_at_risk</span><span class="p">,</span> \
        <span class="n">conditional_value_at_risk_subgradient</span>
    <span class="n">num_design_pts</span><span class="p">,</span> <span class="n">num_design_factors</span> <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">num_pred_pts</span><span class="p">,</span>   <span class="n">num_pred_factors</span>   <span class="o">=</span> <span class="n">pred_factors</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">design_prob_measure</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">design_prob_measure</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span>
        <span class="n">design_prob_measure</span> <span class="o">=</span> <span class="n">design_prob_measure</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">M0</span><span class="p">,</span><span class="n">M1</span><span class="o">=</span><span class="n">get_M0_and_M1_matrices</span><span class="p">(</span>
        <span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_prob_measure</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="n">regression_type</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">noise_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">Q</span><span class="p">,</span><span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">M1</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">pred_factors</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
        <span class="n">M0u</span> <span class="o">=</span> <span class="n">M0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
        <span class="n">variances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span><span class="o">*</span><span class="n">M0u</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">conditional_value_at_risk</span><span class="p">(</span><span class="n">variances</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">return_grad</span><span class="p">):</span>
            <span class="n">gamma</span>   <span class="o">=</span> <span class="o">-</span><span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="p">,(</span><span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">M0u</span><span class="p">)))</span>
            <span class="n">Fu</span>  <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
            <span class="n">t</span>   <span class="o">=</span> <span class="n">noise_multiplier</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">Fu</span>
            <span class="n">Fgamma</span>  <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
            <span class="n">cvar_grad</span> <span class="o">=</span> <span class="n">conditional_value_at_risk_subgradient</span><span class="p">(</span><span class="n">variances</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;lstsq&#39;</span><span class="p">:</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">Fu</span><span class="o">*</span><span class="n">Fgamma</span><span class="o">+</span><span class="n">t</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">cvar_grad</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">Fu</span><span class="o">*</span><span class="n">Fgamma</span><span class="o">/</span><span class="n">noise_multiplier</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span><span class="o">+</span><span class="n">Fu</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">cvar_grad</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                
            <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">gradient</span><span class="o">.</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">value</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">M1</span><span class="p">,</span><span class="n">pred_factors</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="c1"># Value</span>
        <span class="c1"># We want to sum the variances, i.e. the enties of the diagonal of</span>
        <span class="c1"># pred_factors.dot(M1.dot(pred_factors.T))</span>
        <span class="c1"># We know that diag(A.T.dot(B)) = (A*B).axis=0)</span>
        <span class="c1"># The following sums over all entries of A*B we get the diagonal</span>
        <span class="c1"># variances</span>
        <span class="n">variances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pred_factors</span><span class="o">*</span><span class="n">u</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">conditional_value_at_risk</span><span class="p">(</span><span class="n">variances</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">return_grad</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">value</span>
        <span class="c1"># Gradient</span>
        <span class="n">F_M1_inv_P</span> <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">);</span>
        <span class="n">cvar_grad</span> <span class="o">=</span> <span class="n">conditional_value_at_risk_subgradient</span><span class="p">(</span><span class="n">variances</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
        <span class="n">gradient</span>   <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">F_M1_inv_P</span><span class="o">.</span><span class="n">T</span><span class="o">**</span><span class="mi">2</span><span class="o">*</span><span class="n">cvar_grad</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">gradient</span><span class="o">.</span><span class="n">T</span></div>

<div class="viewcode-block" id="goptimality_criterion"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.goptimality_criterion.html#pyapprox.optimal_experimental_design.goptimality_criterion">[docs]</a><span class="k">def</span> <span class="nf">goptimality_criterion</span><span class="p">(</span><span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span>
                          <span class="n">pred_factors</span><span class="p">,</span><span class="n">design_prob_measure</span><span class="p">,</span><span class="n">return_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">noise_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">regression_type</span><span class="o">=</span><span class="s1">&#39;lstsq&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    valuate the G-optimality criterion for a given design probability measure </span>
<span class="sd">    for the linear model</span>

<span class="sd">    .. math::  y(x) = F(x)\theta+\eta(x)\epsilon.</span>

<span class="sd">    The criteria is</span>

<span class="sd">    .. math:: \max{sup}_{xi\in\Xi_\text{pred}} \sigma^2f(x)^T\left(F(\mathcal{X})^TF(\mathcal{X})\right)^{-1}f(x)</span>

<span class="sd">    where</span>

<span class="sd">    .. math:: C(\mu) = M_1^{-1} M_0 M^{-1}</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    homog_outer_prods : np.ndarray (num_design_factors,num_design_factors,</span>
<span class="sd">                                    num_design_pts)</span>
<span class="sd">       The hessian M_1 of the error for each design point</span>

<span class="sd">    design_factors : np.ndarray (num_design_pts,num_design_factors)</span>
<span class="sd">       The design factors evaluated at each of the design points</span>

<span class="sd">    pred_factors : np.ndarray (num_pred_pts,num_pred_factors)</span>
<span class="sd">       The prediction factors g evaluated at each of the prediction points</span>

<span class="sd">    design_prob_measure : np.ndarray (num_design_pts)</span>
<span class="sd">       The prob measure :math:`\mu` on the design points</span>

<span class="sd">    return_grad : boolean</span>
<span class="sd">       True  - return the value and gradient of the criterion</span>
<span class="sd">       False - return only the value of the criterion</span>

<span class="sd">    noise_multiplier : np.ndarray (num_design_pts)</span>
<span class="sd">        The design dependent noise function :math:`\eta(x)`</span>

<span class="sd">    regression_type : string</span>
<span class="sd">        The method used to compute the coefficients of the linear model. </span>
<span class="sd">        Currently supported options are ``lstsq`` and ``quantile``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    value : np.ndarray (num_pred_pts)</span>
<span class="sd">        The value of the objective function</span>

<span class="sd">    grad : np.ndarray (num_pred_pts,num_design_pts)</span>
<span class="sd">        The gradient of the objective function. Only if return_grad is True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_design_pts</span><span class="p">,</span> <span class="n">num_design_factors</span> <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">num_pred_pts</span><span class="p">,</span>   <span class="n">num_pred_factors</span>   <span class="o">=</span> <span class="n">pred_factors</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">design_prob_measure</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">design_prob_measure</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span>
        <span class="n">design_prob_measure</span> <span class="o">=</span> <span class="n">design_prob_measure</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">M0</span><span class="p">,</span><span class="n">M1</span><span class="o">=</span><span class="n">get_M0_and_M1_matrices</span><span class="p">(</span>
        <span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_prob_measure</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="n">regression_type</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">noise_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">Q</span><span class="p">,</span><span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">M1</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">pred_factors</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
        <span class="n">M0u</span> <span class="o">=</span> <span class="n">M0</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
        <span class="n">variances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span><span class="o">*</span><span class="n">M0u</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">variances</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">return_grad</span><span class="p">):</span>
            <span class="n">gamma</span>   <span class="o">=</span> <span class="o">-</span><span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="p">,(</span><span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">M0u</span><span class="p">)))</span>
            <span class="n">Fu</span>  <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
            <span class="n">t</span>   <span class="o">=</span> <span class="n">noise_multiplier</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">Fu</span>
            <span class="n">Fgamma</span>  <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;lstsq&#39;</span><span class="p">:</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">Fu</span><span class="o">*</span><span class="n">Fgamma</span> <span class="o">+</span> <span class="n">t</span><span class="o">**</span><span class="mi">2</span>
            <span class="k">elif</span> <span class="n">regression_type</span><span class="o">==</span><span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">Fu</span><span class="o">*</span><span class="n">Fgamma</span><span class="o">/</span><span class="n">noise_multiplier</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="n">Fu</span><span class="o">**</span><span class="mi">2</span>
            <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">gradient</span><span class="o">.</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">value</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">M1</span><span class="p">,</span><span class="n">pred_factors</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="c1"># Value</span>
        <span class="c1"># We want to sum the variances, i.e. the enties of the diagonal of</span>
        <span class="c1"># pred_factors.dot(M1.dot(pred_factors.T))</span>
        <span class="c1"># We know that diag(A.T.dot(B)) = (A*B).axis=0)00</span>
        <span class="c1"># The following sums over all entries of A*B we get the diagonal</span>
        <span class="c1"># variances</span>
        <span class="n">variances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pred_factors</span><span class="o">*</span><span class="n">u</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">variances</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">return_grad</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">value</span>
        <span class="c1"># Gradient</span>
        <span class="n">F_M1_inv_P</span> <span class="o">=</span> <span class="n">design_factors</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">);</span>
        <span class="n">gradient</span>   <span class="o">=</span> <span class="o">-</span><span class="n">F_M1_inv_P</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">gradient</span><span class="o">.</span><span class="n">T</span></div>

<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">Bounds</span><span class="p">,</span> <span class="n">minimize</span><span class="p">,</span> <span class="n">LinearConstraint</span><span class="p">,</span> <span class="n">NonlinearConstraint</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<div class="viewcode-block" id="minimax_oed_objective"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.minimax_oed_objective.html#pyapprox.optimal_experimental_design.minimax_oed_objective">[docs]</a><span class="k">def</span> <span class="nf">minimax_oed_objective</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>

<div class="viewcode-block" id="minimax_oed_objective_jacobian"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.minimax_oed_objective_jacobian.html#pyapprox.optimal_experimental_design.minimax_oed_objective_jacobian">[docs]</a><span class="k">def</span> <span class="nf">minimax_oed_objective_jacobian</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">vec</span></div>

<div class="viewcode-block" id="minimax_oed_constraint_objective"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.minimax_oed_constraint_objective.html#pyapprox.optimal_experimental_design.minimax_oed_constraint_objective">[docs]</a><span class="k">def</span> <span class="nf">minimax_oed_constraint_objective</span><span class="p">(</span><span class="n">local_oed_obj</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">local_oed_obj</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span></div>

<div class="viewcode-block" id="minimax_oed_constraint_jacobian"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.minimax_oed_constraint_jacobian.html#pyapprox.optimal_experimental_design.minimax_oed_constraint_jacobian">[docs]</a><span class="k">def</span> <span class="nf">minimax_oed_constraint_jacobian</span><span class="p">(</span><span class="n">local_oed_jac</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
    <span class="n">jac</span> <span class="o">=</span> <span class="o">-</span><span class="n">local_oed_jac</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">jac</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">jac</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">jac</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span><span class="n">jac</span><span class="p">])</span></div>

<div class="viewcode-block" id="get_minimax_bounds"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.get_minimax_bounds.html#pyapprox.optimal_experimental_design.get_minimax_bounds">[docs]</a><span class="k">def</span> <span class="nf">get_minimax_bounds</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Bounds</span><span class="p">(</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">num_design_pts</span><span class="p">,[</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">num_design_pts</span><span class="p">)</span></div>

<div class="viewcode-block" id="get_minimax_default_initial_guess"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.get_minimax_default_initial_guess.html#pyapprox.optimal_experimental_design.get_minimax_default_initial_guess">[docs]</a><span class="k">def</span> <span class="nf">get_minimax_default_initial_guess</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">):</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_design_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">num_design_pts</span>
    <span class="n">x0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">x0</span></div>

<div class="viewcode-block" id="get_minimax_linear_constraints"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.get_minimax_linear_constraints.html#pyapprox.optimal_experimental_design.get_minimax_linear_constraints">[docs]</a><span class="k">def</span> <span class="nf">get_minimax_linear_constraints</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">):</span>
    <span class="n">lb_con</span> <span class="o">=</span> <span class="n">ub_con</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">A_con</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">num_design_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">A_con</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">LinearConstraint</span><span class="p">(</span><span class="n">A_con</span><span class="p">,</span> <span class="n">lb_con</span><span class="p">,</span> <span class="n">ub_con</span><span class="p">)</span></div>

<div class="viewcode-block" id="extract_minimax_design_from_optimize_result"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.extract_minimax_design_from_optimize_result.html#pyapprox.optimal_experimental_design.extract_minimax_design_from_optimize_result">[docs]</a><span class="k">def</span> <span class="nf">extract_minimax_design_from_optimize_result</span><span class="p">(</span><span class="n">res</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span></div>

<div class="viewcode-block" id="r_oed_objective"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.r_oed_objective.html#pyapprox.optimal_experimental_design.r_oed_objective">[docs]</a><span class="k">def</span> <span class="nf">r_oed_objective</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span><span class="n">pred_weights</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
    <span class="n">num_pred_pts</span> <span class="o">=</span> <span class="n">pred_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span><span class="o">*</span><span class="n">u</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">pred_weights</span><span class="p">)</span></div>

<div class="viewcode-block" id="r_oed_objective_jacobian"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.r_oed_objective_jacobian.html#pyapprox.optimal_experimental_design.r_oed_objective_jacobian">[docs]</a><span class="k">def</span> <span class="nf">r_oed_objective_jacobian</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span><span class="n">pred_weights</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
    <span class="n">num_pred_pts</span> <span class="o">=</span> <span class="n">pred_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred_weights</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">vec</span></div>

<div class="viewcode-block" id="r_oed_constraint_objective"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.r_oed_constraint_objective.html#pyapprox.optimal_experimental_design.r_oed_constraint_objective">[docs]</a><span class="k">def</span> <span class="nf">r_oed_constraint_objective</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">,</span><span class="n">local_oed_obj</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
    <span class="n">num_pred_pts</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">num_design_pts</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">t</span><span class="o">+</span><span class="n">u</span><span class="o">-</span><span class="n">local_oed_obj</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">:])</span><span class="o">.</span><span class="n">T</span></div>

<div class="viewcode-block" id="r_oed_constraint_jacobian"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.r_oed_constraint_jacobian.html#pyapprox.optimal_experimental_design.r_oed_constraint_jacobian">[docs]</a><span class="k">def</span> <span class="nf">r_oed_constraint_jacobian</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">,</span><span class="n">local_oed_jac</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
    <span class="n">num_pred_pts</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">num_design_pts</span><span class="p">)</span>
    <span class="n">jac</span> <span class="o">=</span> <span class="o">-</span><span class="n">local_oed_jac</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">:])</span>
    <span class="k">assert</span> <span class="n">jac</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">jac</span><span class="o">.</span><span class="n">shape</span><span class="o">==</span><span class="p">(</span><span class="n">num_pred_pts</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">)</span>
    <span class="n">jac</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">jac</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">jac</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">num_pred_pts</span><span class="p">),</span><span class="n">jac</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">jac</span></div>

<div class="viewcode-block" id="r_oed_sparse_constraint_jacobian"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.r_oed_sparse_constraint_jacobian.html#pyapprox.optimal_experimental_design.r_oed_sparse_constraint_jacobian">[docs]</a><span class="k">def</span> <span class="nf">r_oed_sparse_constraint_jacobian</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">,</span><span class="n">local_oed_jac</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
    <span class="n">num_pred_pts</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">num_design_pts</span><span class="p">)</span>
    <span class="n">local_jac</span> <span class="o">=</span> <span class="o">-</span><span class="n">local_oed_jac</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">:])</span>
    <span class="k">assert</span> <span class="n">local_jac</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">2</span> <span class="ow">and</span> <span class="n">local_jac</span><span class="o">.</span><span class="n">shape</span><span class="o">==</span><span class="p">(</span><span class="n">num_pred_pts</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">)</span>
    <span class="n">size</span><span class="o">=</span><span class="n">num_pred_pts</span><span class="o">*</span><span class="n">num_design_pts</span><span class="o">+</span><span class="mi">1</span>
    <span class="n">jac</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">num_pred_pts</span><span class="p">,</span><span class="mi">2</span><span class="o">+</span><span class="n">num_design_pts</span><span class="p">))</span>
    <span class="n">jac</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
    <span class="n">jac</span><span class="p">[:,</span><span class="mi">2</span><span class="p">:]</span><span class="o">=</span><span class="n">local_jac</span>
    <span class="n">jac</span> <span class="o">=</span> <span class="n">jac</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">jac</span></div>

<div class="viewcode-block" id="get_r_oed_bounds"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.get_r_oed_bounds.html#pyapprox.optimal_experimental_design.get_r_oed_bounds">[docs]</a><span class="k">def</span> <span class="nf">get_r_oed_bounds</span><span class="p">(</span><span class="n">num_pred_pts</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Bounds</span><span class="p">(</span>
        <span class="p">[</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="n">num_design_pts</span><span class="p">),[</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">))</span></div>

<div class="viewcode-block" id="get_r_oed_default_initial_guess"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.get_r_oed_default_initial_guess.html#pyapprox.optimal_experimental_design.get_r_oed_default_initial_guess">[docs]</a><span class="k">def</span> <span class="nf">get_r_oed_default_initial_guess</span><span class="p">(</span><span class="n">num_pred_pts</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">):</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="n">num_design_pts</span><span class="p">)</span><span class="o">/</span><span class="n">num_design_pts</span>
    <span class="n">x0</span><span class="p">[:</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">x0</span></div>

<div class="viewcode-block" id="get_r_oed_linear_constraints"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.get_r_oed_linear_constraints.html#pyapprox.optimal_experimental_design.get_r_oed_linear_constraints">[docs]</a><span class="k">def</span> <span class="nf">get_r_oed_linear_constraints</span><span class="p">(</span><span class="n">num_pred_pts</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">):</span>
    <span class="n">lb_con</span> <span class="o">=</span> <span class="n">ub_con</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">A_con</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="o">+</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="n">num_design_pts</span><span class="p">))</span>
    <span class="n">A_con</span><span class="p">[</span><span class="mi">0</span><span class="p">,:</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">LinearConstraint</span><span class="p">(</span><span class="n">A_con</span><span class="p">,</span> <span class="n">lb_con</span><span class="p">,</span> <span class="n">ub_con</span><span class="p">)</span></div>

<div class="viewcode-block" id="extract_r_oed_design_from_optimize_result"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.extract_r_oed_design_from_optimize_result.html#pyapprox.optimal_experimental_design.extract_r_oed_design_from_optimize_result">[docs]</a><span class="k">def</span> <span class="nf">extract_r_oed_design_from_optimize_result</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">,</span><span class="n">res</span><span class="p">):</span>
    <span class="n">num_pred_pts</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">num_design_pts</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span></div>

<div class="viewcode-block" id="get_r_oed_jacobian_structure"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.get_r_oed_jacobian_structure.html#pyapprox.optimal_experimental_design.get_r_oed_jacobian_structure">[docs]</a><span class="k">def</span> <span class="nf">get_r_oed_jacobian_structure</span><span class="p">(</span><span class="n">num_pred_pts</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">):</span>
    <span class="n">nonlinear_constraint_structure</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_pred_pts</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">num_pred_pts</span><span class="p">,</span><span class="n">num_pred_pts</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span>
            <span class="n">num_pred_pts</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">))])</span>
    <span class="n">linear_constraint_structure</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">num_pred_pts</span><span class="o">+</span><span class="n">num_design_pts</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">linear_constraint_structure</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="o">+</span><span class="n">num_pred_pts</span><span class="p">:]</span><span class="o">=</span><span class="mi">1</span>
    <span class="n">structure</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">linear_constraint_structure</span><span class="p">,</span><span class="n">nonlinear_constraint_structure</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">structure</span><span class="p">)</span></div>

<div class="viewcode-block" id="AlphabetOptimalDesign"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.AlphabetOptimalDesign.html#pyapprox.optimal_experimental_design.AlphabetOptimalDesign">[docs]</a><span class="k">class</span> <span class="nc">AlphabetOptimalDesign</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">        # Even though scipy.optimize.minimize may print the warning</span>
<span class="sd">        # UserWarning: delta_grad == 0.0. Check if the approximated function is</span>
<span class="sd">        # linear. If the function is linear better results can be obtained by</span>
<span class="sd">        # defining the Hessian as zero instead of using quasi-Newton</span>
<span class="sd">        # approximations.</span>
<span class="sd">        # The Hessian is not zero.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">criteria</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">opts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">regression_type</span><span class="o">=</span><span class="s1">&#39;lstsq&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criteria</span><span class="o">=</span><span class="n">criteria</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">noise_multiplier</span> <span class="o">=</span> <span class="n">noise_multiplier</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">design_factors</span> <span class="o">=</span> <span class="n">design_factors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opts</span><span class="o">=</span><span class="n">opts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">regression_type</span><span class="o">=</span><span class="n">regression_type</span>
        
<div class="viewcode-block" id="AlphabetOptimalDesign.get_objective_and_jacobian"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.AlphabetOptimalDesign.html#pyapprox.optimal_experimental_design.AlphabetOptimalDesign.get_objective_and_jacobian">[docs]</a>    <span class="k">def</span> <span class="nf">get_objective_and_jacobian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span><span class="n">homog_outer_prods</span><span class="p">,</span>
                                   <span class="n">noise_multiplier</span><span class="p">,</span><span class="n">opts</span><span class="p">):</span>

        <span class="c1">#criteria requiring pred_factors</span>
        <span class="n">pred_criteria_funcs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;G&#39;</span><span class="p">:</span><span class="n">goptimality_criterion</span><span class="p">,</span><span class="s1">&#39;I&#39;</span><span class="p">:</span><span class="n">ioptimality_criterion</span><span class="p">}</span>
        <span class="c1">#criteria not requiring pred_factors</span>
        <span class="n">other_criteria_funcs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;A&#39;</span><span class="p">:</span><span class="n">aoptimality_criterion</span><span class="p">,</span><span class="s1">&#39;C&#39;</span><span class="p">:</span><span class="n">coptimality_criterion</span><span class="p">,</span>
            <span class="s1">&#39;D&#39;</span><span class="p">:</span><span class="n">doptimality_criterion</span><span class="p">}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criteria</span> <span class="ow">in</span> <span class="n">other_criteria_funcs</span><span class="p">:</span>
            <span class="n">criteria_fun</span> <span class="o">=</span>  <span class="n">other_criteria_funcs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">criteria</span><span class="p">]</span>
            <span class="n">objective</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">criteria_fun</span><span class="p">,</span><span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span>
                <span class="n">noise_multiplier</span><span class="o">=</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="n">return_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">regression_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regression_type</span><span class="p">)</span>
            <span class="n">jac</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">criteria_fun</span><span class="p">(</span>
                <span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">return_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">noise_multiplier</span><span class="o">=</span><span class="n">noise_multiplier</span><span class="p">,</span>
                <span class="n">regression_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regression_type</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">criteria</span> <span class="ow">in</span> <span class="n">pred_criteria_funcs</span><span class="p">:</span>
            <span class="n">criteria_fun</span> <span class="o">=</span>  <span class="n">pred_criteria_funcs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">criteria</span><span class="p">]</span>
            <span class="n">pred_factors</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s1">&#39;pred_factors&#39;</span><span class="p">]</span>
            <span class="n">objective</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">criteria_fun</span><span class="p">,</span><span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span>
                <span class="n">pred_factors</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="o">=</span><span class="n">noise_multiplier</span><span class="p">,</span>
                <span class="n">return_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">regression_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regression_type</span><span class="p">)</span>
            <span class="n">jac</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">criteria_fun</span><span class="p">(</span>
                <span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span><span class="n">pred_factors</span><span class="p">,</span><span class="n">r</span><span class="p">,</span>
                <span class="n">return_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="o">=</span><span class="n">noise_multiplier</span><span class="p">,</span>
                <span class="n">regression_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regression_type</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">criteria</span><span class="o">==</span><span class="s1">&#39;R&#39;</span><span class="p">:</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span>
            <span class="n">pred_factors</span> <span class="o">=</span> <span class="n">opts</span><span class="p">[</span><span class="s1">&#39;pred_factors&#39;</span><span class="p">]</span>
            <span class="n">objective</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">roptimality_criterion</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="n">homog_outer_prods</span><span class="p">,</span>
                <span class="n">design_factors</span><span class="p">,</span><span class="n">pred_factors</span><span class="p">,</span>
                <span class="n">noise_multiplier</span><span class="o">=</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="n">return_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">regression_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regression_type</span><span class="p">)</span>
            <span class="n">jac</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">roptimality_criterion</span><span class="p">(</span>
                <span class="n">beta</span><span class="p">,</span><span class="n">homog_outer_prods</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span><span class="n">pred_factors</span><span class="p">,</span><span class="n">r</span><span class="p">,</span>
                <span class="n">return_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="o">=</span><span class="n">noise_multiplier</span><span class="p">,</span>
                <span class="n">regression_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regression_type</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Optimality criteria: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">criteria</span><span class="si">}</span><span class="s1"> is not supported. &#39;</span>
            <span class="n">msg</span> <span class="o">+=</span> <span class="s1">&#39;Supported criteria are:</span><span class="se">\n</span><span class="s1">&#39;</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">other_criteria_funcs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">msg</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">pred_criteria_funcs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">msg</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="n">msg</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">R</span><span class="se">\n</span><span class="s1">&#39;</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">objective</span><span class="p">,</span><span class="n">jac</span></div>
        

<div class="viewcode-block" id="AlphabetOptimalDesign.solve"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.AlphabetOptimalDesign.html#pyapprox.optimal_experimental_design.AlphabetOptimalDesign.solve">[docs]</a>    <span class="k">def</span> <span class="nf">solve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">init_design</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">return_full</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">num_design_pts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">design_factors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">homog_outer_prods</span> <span class="o">=</span> <span class="n">compute_homoscedastic_outer_products</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">design_factors</span><span class="p">)</span>

        <span class="n">objective</span><span class="p">,</span><span class="n">jac</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_objective_and_jacobian</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">design_factors</span><span class="p">,</span><span class="n">homog_outer_prods</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">opts</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criteria</span><span class="o">==</span><span class="s1">&#39;G&#39;</span><span class="p">:</span> 
            <span class="n">constraint_obj</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">minimax_oed_constraint_objective</span><span class="p">,</span><span class="n">objective</span><span class="p">)</span>
            <span class="n">constraint_jac</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">minimax_oed_constraint_jacobian</span><span class="p">,</span><span class="n">jac</span><span class="p">)</span>
            <span class="n">nonlinear_constraints</span> <span class="o">=</span> <span class="p">[</span><span class="n">NonlinearConstraint</span><span class="p">(</span>
                <span class="n">constraint_obj</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span><span class="n">jac</span><span class="o">=</span><span class="n">constraint_jac</span><span class="p">)]</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_minimax</span><span class="p">(</span>
                <span class="n">nonlinear_constraints</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">,</span><span class="n">options</span><span class="p">,</span><span class="n">return_full</span><span class="p">,</span>
                <span class="n">init_design</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">criteria</span><span class="o">==</span><span class="s1">&#39;R&#39;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">opts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;nonsmooth&#39;</span><span class="p">,</span><span class="kc">False</span><span class="p">):</span>
            <span class="c1">#if nonsmooth is True then minimize then constraint objective</span>
            <span class="c1">#(objective returned self.get_objective_and_jacobian) will not be</span>
            <span class="c1"># differentiable instead and the jacobian returned will consists of sub-differentials and this will be passed to minimize and this section skipped</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">criteria</span><span class="o">=</span><span class="s1">&#39;G&#39;</span>
            <span class="n">objective</span><span class="p">,</span><span class="n">jac</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_objective_and_jacobian</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">design_factors</span><span class="p">,</span><span class="n">homog_outer_prods</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">opts</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">criteria</span><span class="o">=</span><span class="s1">&#39;R&#39;</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opts</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span>
            <span class="n">num_pred_pts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">opts</span><span class="p">[</span><span class="s1">&#39;pred_factors&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_pred_pts</span><span class="p">)</span><span class="o">/</span><span class="n">num_pred_pts</span>
            <span class="k">assert</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">num_pred_pts</span>
            <span class="n">r_obj</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">r_oed_objective</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="n">weights</span><span class="p">)</span>
            <span class="n">r_jac</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">r_oed_objective_jacobian</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="n">weights</span><span class="p">)</span>
            <span class="n">constraint_obj</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">r_oed_constraint_objective</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">,</span><span class="n">objective</span><span class="p">)</span>
            <span class="n">constraint_jac</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">r_oed_constraint_jacobian</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">,</span><span class="n">jac</span><span class="p">)</span>
            <span class="n">nonlinear_constraints</span> <span class="o">=</span> <span class="p">[</span><span class="n">NonlinearConstraint</span><span class="p">(</span>
                <span class="n">constraint_obj</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span><span class="n">jac</span><span class="o">=</span><span class="n">constraint_jac</span><span class="p">)]</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_minimax</span><span class="p">(</span>
                <span class="n">nonlinear_constraints</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">,</span><span class="n">options</span><span class="p">,</span><span class="n">return_full</span><span class="p">,</span>
                <span class="n">init_design</span><span class="p">,</span><span class="n">objective</span><span class="o">=</span><span class="n">r_obj</span><span class="p">,</span><span class="n">jac</span><span class="o">=</span><span class="n">r_jac</span><span class="p">,</span>
                <span class="n">get_bounds</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">get_r_oed_bounds</span><span class="p">,</span><span class="n">num_pred_pts</span><span class="p">),</span>
                <span class="n">get_init_guess</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">get_r_oed_default_initial_guess</span><span class="p">,</span><span class="n">num_pred_pts</span><span class="p">),</span>
                <span class="n">get_linear_constraint</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">get_r_oed_linear_constraints</span><span class="p">,</span><span class="n">num_pred_pts</span><span class="p">),</span>
                <span class="n">extract_design_from_optimize_result</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">extract_r_oed_design_from_optimize_result</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bounds</span> <span class="o">=</span> <span class="n">Bounds</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">num_design_pts</span><span class="p">,[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">num_design_pts</span><span class="p">)</span>
        <span class="n">lb_con</span> <span class="o">=</span> <span class="n">ub_con</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">A_con</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_constraint</span> <span class="o">=</span> <span class="n">LinearConstraint</span><span class="p">(</span><span class="n">A_con</span><span class="p">,</span> <span class="n">lb_con</span><span class="p">,</span> <span class="n">ub_con</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">init_design</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">)</span><span class="o">/</span><span class="n">num_design_pts</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x0</span> <span class="o">=</span> <span class="n">init_design</span>

        <span class="k">if</span> <span class="s1">&#39;solver&#39;</span> <span class="ow">in</span> <span class="n">options</span><span class="p">:</span>
            <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">method</span> <span class="o">=</span> <span class="n">options</span><span class="p">[</span><span class="s1">&#39;solver&#39;</span><span class="p">]</span>
            <span class="k">del</span> <span class="n">options</span><span class="p">[</span><span class="s1">&#39;solver&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1">#method=&#39;trust-constr&#39;</span>
            <span class="n">method</span><span class="o">=</span><span class="s1">&#39;slsqp&#39;</span>

        <span class="k">if</span> <span class="n">method</span><span class="o">==</span><span class="s1">&#39;ipopt&#39;</span><span class="p">:</span>
            <span class="c1"># when printing results of derivative_test The first floating point</span>
            <span class="c1"># number is the value given by the user code, and the second number</span>
            <span class="c1"># (after &quot;~&quot;) is the finite differences estimation. Finally, the</span>
            <span class="c1"># number in square brackets is the relative difference between</span>
            <span class="c1"># these two numbers.</span>
            <span class="n">bounds</span> <span class="o">=</span> <span class="p">[[</span><span class="n">lb</span><span class="p">,</span><span class="n">ub</span><span class="p">]</span> <span class="k">for</span> <span class="n">lb</span><span class="p">,</span><span class="n">ub</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="o">.</span><span class="n">lb</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="o">.</span><span class="n">ub</span><span class="p">)]</span>
            <span class="kn">from</span> <span class="nn">scipy.optimize._constraints</span> <span class="kn">import</span> <span class="n">new_constraint_to_old</span>
            <span class="n">con</span> <span class="o">=</span> <span class="n">new_constraint_to_old</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_constraint</span><span class="p">,</span><span class="n">x0</span><span class="p">)</span>
            <span class="kn">from</span> <span class="nn">ipopt</span> <span class="kn">import</span> <span class="n">minimize_ipopt</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">minimize_ipopt</span><span class="p">(</span>
                <span class="n">objective</span><span class="p">,</span><span class="n">x0</span><span class="p">,</span><span class="n">jac</span><span class="o">=</span><span class="n">jac</span><span class="p">,</span><span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span><span class="n">constraints</span><span class="o">=</span><span class="n">con</span><span class="p">,</span>
                <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span>
                <span class="n">objective</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">jac</span><span class="p">,</span> <span class="n">hess</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">constraints</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_constraint</span><span class="p">],</span><span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
                <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span>
            
        <span class="n">weights</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_full</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">weights</span>
        
        <span class="k">return</span> <span class="n">weights</span><span class="p">,</span><span class="n">res</span></div>

<div class="viewcode-block" id="AlphabetOptimalDesign.minimax_nonlinear_constraints"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.AlphabetOptimalDesign.html#pyapprox.optimal_experimental_design.AlphabetOptimalDesign.minimax_nonlinear_constraints">[docs]</a>    <span class="k">def</span> <span class="nf">minimax_nonlinear_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">parameter_samples</span><span class="p">,</span><span class="n">design_samples</span><span class="p">):</span>
        <span class="n">constraints</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">parameter_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">design_factors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">design_factors</span><span class="p">(</span>
                <span class="n">parameter_samples</span><span class="p">[:,</span><span class="n">ii</span><span class="p">],</span><span class="n">design_samples</span><span class="p">)</span>
            <span class="n">homog_outer_prods</span> <span class="o">=</span> <span class="n">compute_homoscedastic_outer_products</span><span class="p">(</span>
                <span class="n">design_factors</span><span class="p">)</span>
            <span class="n">opts</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">opts</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">opts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s1">&#39;pred_factors&#39;</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">:</span>
                <span class="n">opts</span><span class="p">[</span><span class="s1">&#39;pred_factors&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s1">&#39;pred_factors&#39;</span><span class="p">](</span>
                    <span class="n">parameter_samples</span><span class="p">[:,</span><span class="n">ii</span><span class="p">],</span><span class="n">opts</span><span class="p">[</span><span class="s1">&#39;pred_samples&#39;</span><span class="p">])</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_multiplier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">noise_multiplier</span><span class="o">=</span><span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">noise_multiplier</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noise_multiplier</span><span class="p">(</span>
                    <span class="n">parameter_samples</span><span class="p">[:,</span><span class="n">ii</span><span class="p">],</span><span class="n">design_samples</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
                <span class="k">assert</span> <span class="n">noise_multiplier</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">1</span>
                <span class="k">assert</span> <span class="n">noise_multiplier</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">design_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">obj</span><span class="p">,</span><span class="n">jac</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_objective_and_jacobian</span><span class="p">(</span>
                <span class="n">design_factors</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span><span class="n">homog_outer_prods</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                <span class="n">noise_multiplier</span><span class="p">,</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">opts</span><span class="p">))</span>
            <span class="n">constraint_obj</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">minimax_oed_constraint_objective</span><span class="p">,</span><span class="n">obj</span><span class="p">)</span>
            <span class="n">constraint_jac</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">minimax_oed_constraint_jacobian</span><span class="p">,</span><span class="n">jac</span><span class="p">)</span>
            <span class="n">constraint</span> <span class="o">=</span> <span class="n">NonlinearConstraint</span><span class="p">(</span>
                <span class="n">constraint_obj</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span><span class="n">jac</span><span class="o">=</span><span class="n">constraint_jac</span><span class="p">)</span>
            <span class="n">constraints</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">constraint</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">constraints</span></div>

    <span class="k">def</span> <span class="nf">_solve_minimax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">nonlinear_constraints</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">,</span><span class="n">options</span><span class="p">,</span>
                       <span class="n">return_full</span><span class="p">,</span><span class="n">x0</span><span class="p">,</span><span class="n">objective</span><span class="o">=</span><span class="n">minimax_oed_objective</span><span class="p">,</span>
                       <span class="n">jac</span><span class="o">=</span><span class="n">minimax_oed_objective_jacobian</span><span class="p">,</span>
                       <span class="n">get_bounds</span><span class="o">=</span><span class="n">get_minimax_bounds</span><span class="p">,</span>
                       <span class="n">get_init_guess</span><span class="o">=</span><span class="n">get_minimax_default_initial_guess</span><span class="p">,</span>
                       <span class="n">get_linear_constraint</span><span class="o">=</span><span class="n">get_minimax_linear_constraints</span><span class="p">,</span>
                       <span class="n">extract_design_from_optimize_result</span><span class="o">=</span><span class="n">extract_minimax_design_from_optimize_result</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_constraint</span> <span class="o">=</span> <span class="n">get_linear_constraint</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">)</span>
        <span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_constraint</span><span class="p">]</span>
        <span class="n">constraints</span> <span class="o">+=</span> <span class="n">nonlinear_constraints</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">bounds</span> <span class="o">=</span> <span class="n">get_bounds</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">x0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x0</span> <span class="o">=</span> <span class="n">get_init_guess</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">)</span>

        <span class="k">if</span> <span class="s1">&#39;solver&#39;</span> <span class="ow">in</span> <span class="n">options</span><span class="p">:</span>
            <span class="n">method</span> <span class="o">=</span> <span class="n">options</span><span class="p">[</span><span class="s1">&#39;solver&#39;</span><span class="p">]</span>
            <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="k">del</span> <span class="n">options</span><span class="p">[</span><span class="s1">&#39;solver&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1">#method=&#39;trust-constr&#39;</span>
            <span class="n">method</span><span class="o">=</span><span class="s1">&#39;slsqp&#39;</span>

        <span class="k">if</span> <span class="n">method</span><span class="o">==</span><span class="s1">&#39;ipopt&#39;</span><span class="p">:</span>
            <span class="n">bounds</span> <span class="o">=</span> <span class="p">[[</span><span class="n">lb</span><span class="p">,</span><span class="n">ub</span><span class="p">]</span> <span class="k">for</span> <span class="n">lb</span><span class="p">,</span><span class="n">ub</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="o">.</span><span class="n">lb</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="o">.</span><span class="n">ub</span><span class="p">)]</span>
            <span class="kn">from</span> <span class="nn">scipy.optimize._constraints</span> <span class="kn">import</span> <span class="n">new_constraint_to_old</span>
            <span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">new_constraint_to_old</span><span class="p">(</span><span class="n">con</span><span class="p">,</span><span class="n">x0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">con</span> <span class="ow">in</span> <span class="n">constraints</span><span class="p">]</span>
            <span class="kn">from</span> <span class="nn">ipopt</span> <span class="kn">import</span> <span class="n">minimize_ipopt</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># if version of ipopt supports it pass in jacobian structure</span>
                <span class="n">options</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="n">constraint_jacobianstructure</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                    <span class="s1">&#39;constraint_jacobianstructure&#39;</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="s1">&#39;constraint_jacobianstructure&#39;</span> <span class="ow">in</span> <span class="n">options</span><span class="p">:</span>
                    <span class="k">del</span> <span class="n">options</span><span class="p">[</span><span class="s1">&#39;constraint_jacobianstructure&#39;</span><span class="p">]</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">minimize_ipopt</span><span class="p">(</span>
                    <span class="n">objective</span><span class="p">,</span><span class="n">x0</span><span class="p">,</span><span class="n">jac</span><span class="o">=</span><span class="n">jac</span><span class="p">,</span><span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
                    <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">,</span>
                    <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
                    <span class="n">constraint_jacobianstructure</span><span class="o">=</span><span class="n">constraint_jacobianstructure</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">minimize_ipopt</span><span class="p">(</span>
                    <span class="n">objective</span><span class="p">,</span><span class="n">x0</span><span class="p">,</span><span class="n">jac</span><span class="o">=</span><span class="n">jac</span><span class="p">,</span><span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span><span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">,</span>
                    <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span>
                <span class="n">objective</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span><span class="n">jac</span><span class="o">=</span><span class="n">jac</span><span class="p">,</span> <span class="n">hess</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">,</span><span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
                <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span>
        
        <span class="n">weights</span> <span class="o">=</span> <span class="n">extract_design_from_optimize_result</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_full</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">res</span>

<div class="viewcode-block" id="AlphabetOptimalDesign.solve_nonlinear_minimax"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.AlphabetOptimalDesign.html#pyapprox.optimal_experimental_design.AlphabetOptimalDesign.solve_nonlinear_minimax">[docs]</a>    <span class="k">def</span> <span class="nf">solve_nonlinear_minimax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">parameter_samples</span><span class="p">,</span><span class="n">design_samples</span><span class="p">,</span>
                                <span class="n">options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">return_full</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">x0</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">design_factors</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">noise_multiplier</span><span class="p">)</span>
        <span class="n">nonlinear_constraints</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">minimax_nonlinear_constraints</span><span class="p">(</span>
            <span class="n">parameter_samples</span><span class="p">,</span><span class="n">design_samples</span><span class="p">)</span>
        <span class="n">num_design_pts</span> <span class="o">=</span> <span class="n">design_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_minimax</span><span class="p">(</span><span class="n">nonlinear_constraints</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">,</span><span class="n">options</span><span class="p">,</span>
                                   <span class="n">return_full</span><span class="p">,</span><span class="n">x0</span><span class="p">)</span></div>

<div class="viewcode-block" id="AlphabetOptimalDesign.bayesian_objective_jacobian_components"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.AlphabetOptimalDesign.html#pyapprox.optimal_experimental_design.AlphabetOptimalDesign.bayesian_objective_jacobian_components">[docs]</a>    <span class="k">def</span> <span class="nf">bayesian_objective_jacobian_components</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span><span class="n">parameter_samples</span><span class="p">,</span><span class="n">design_samples</span><span class="p">):</span>
        <span class="n">objs</span><span class="p">,</span><span class="n">jacs</span><span class="o">=</span><span class="p">[],[]</span>
        <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">parameter_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">design_factors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">design_factors</span><span class="p">(</span>
                <span class="n">parameter_samples</span><span class="p">[:,</span><span class="n">ii</span><span class="p">],</span><span class="n">design_samples</span><span class="p">)</span>
            <span class="n">homog_outer_prods</span> <span class="o">=</span> <span class="n">compute_homoscedastic_outer_products</span><span class="p">(</span>
                <span class="n">design_factors</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_multiplier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">noise_multiplier</span><span class="o">=</span><span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">noise_multiplier</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noise_multiplier</span><span class="p">(</span>
                    <span class="n">parameter_samples</span><span class="p">[:,</span><span class="n">ii</span><span class="p">],</span><span class="n">design_samples</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
                <span class="k">assert</span> <span class="n">noise_multiplier</span><span class="o">.</span><span class="n">ndim</span><span class="o">==</span><span class="mi">1</span>
                <span class="k">assert</span> <span class="n">noise_multiplier</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">design_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">opts</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">opts</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">opts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s1">&#39;pred_factors&#39;</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">:</span>
                <span class="n">opts</span><span class="p">[</span><span class="s1">&#39;pred_factors&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s1">&#39;pred_factors&#39;</span><span class="p">](</span>
                    <span class="n">parameter_samples</span><span class="p">[:,</span><span class="n">ii</span><span class="p">],</span><span class="n">opts</span><span class="p">[</span><span class="s1">&#39;pred_samples&#39;</span><span class="p">])</span>
            <span class="n">obj</span><span class="p">,</span><span class="n">jac</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_objective_and_jacobian</span><span class="p">(</span>
                <span class="n">design_factors</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span><span class="n">homog_outer_prods</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                <span class="n">noise_multiplier</span><span class="p">,</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">opts</span><span class="p">))</span>
            <span class="n">objs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
            <span class="n">jacs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jac</span><span class="p">)</span>
            
        <span class="n">num_design_pts</span> <span class="o">=</span> <span class="n">homog_outer_prods</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">objs</span><span class="p">,</span><span class="n">jacs</span><span class="p">,</span><span class="n">num_design_pts</span></div>

<div class="viewcode-block" id="AlphabetOptimalDesign.solve_nonlinear_bayesian"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.AlphabetOptimalDesign.html#pyapprox.optimal_experimental_design.AlphabetOptimalDesign.solve_nonlinear_bayesian">[docs]</a>    <span class="k">def</span> <span class="nf">solve_nonlinear_bayesian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">samples</span><span class="p">,</span><span class="n">design_samples</span><span class="p">,</span>
                                 <span class="n">sample_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">return_full</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">x0</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">design_factors</span><span class="p">)</span>
        <span class="n">objs</span><span class="p">,</span><span class="n">jacs</span><span class="p">,</span><span class="n">num_design_pts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bayesian_objective_jacobian_components</span><span class="p">(</span>
            <span class="n">samples</span><span class="p">,</span><span class="n">design_samples</span><span class="p">)</span>
        <span class="n">lb_con</span> <span class="o">=</span> <span class="n">ub_con</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">A_con</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">num_design_pts</span><span class="p">))</span>
        <span class="n">linear_constraint</span> <span class="o">=</span> <span class="n">LinearConstraint</span><span class="p">(</span><span class="n">A_con</span><span class="p">,</span> <span class="n">lb_con</span><span class="p">,</span> <span class="n">ub_con</span><span class="p">)</span>
        <span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span><span class="n">linear_constraint</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">sample_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sample_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                <span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">sample_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">objective</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">obj</span><span class="p">,</span><span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">objs</span><span class="p">,</span><span class="n">sample_weights</span><span class="p">):</span>
                <span class="n">objective</span> <span class="o">+=</span> <span class="n">obj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">weight</span>
            <span class="k">return</span> <span class="n">objective</span>
            
        <span class="k">def</span> <span class="nf">jacobian</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">vec</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">jac</span><span class="p">,</span><span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">jacs</span><span class="p">,</span><span class="n">sample_weights</span><span class="p">):</span>
                <span class="n">vec</span> <span class="o">+=</span> <span class="n">jac</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">weight</span>
            <span class="k">return</span> <span class="n">vec</span>
                
        <span class="n">bounds</span> <span class="o">=</span> <span class="n">Bounds</span><span class="p">(</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">num_design_pts</span><span class="p">,[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">num_design_pts</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">x0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_design_pts</span><span class="p">)</span><span class="o">/</span><span class="n">num_design_pts</span>
            
        <span class="k">if</span> <span class="s1">&#39;solver&#39;</span> <span class="ow">in</span> <span class="n">options</span><span class="p">:</span>
            <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">method</span> <span class="o">=</span> <span class="n">options</span><span class="p">[</span><span class="s1">&#39;solver&#39;</span><span class="p">]</span>
            <span class="k">del</span> <span class="n">options</span><span class="p">[</span><span class="s1">&#39;solver&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1">#method=&#39;trust-constr&#39;</span>
            <span class="n">method</span><span class="o">=</span><span class="s1">&#39;slsqp&#39;</span>
            
        <span class="k">if</span> <span class="n">method</span><span class="o">==</span><span class="s1">&#39;ipopt&#39;</span><span class="p">:</span>
            <span class="n">bounds</span> <span class="o">=</span> <span class="p">[[</span><span class="n">lb</span><span class="p">,</span><span class="n">ub</span><span class="p">]</span> <span class="k">for</span> <span class="n">lb</span><span class="p">,</span><span class="n">ub</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="o">.</span><span class="n">lb</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="o">.</span><span class="n">ub</span><span class="p">)]</span>
            <span class="kn">from</span> <span class="nn">ipopt</span> <span class="kn">import</span> <span class="n">minimize_ipopt</span>
            <span class="kn">from</span> <span class="nn">scipy.optimize._constraints</span> <span class="kn">import</span> <span class="n">new_constraint_to_old</span>
            <span class="n">con</span> <span class="o">=</span> <span class="n">new_constraint_to_old</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_constraint</span><span class="p">,</span><span class="n">x0</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">minimize_ipopt</span><span class="p">(</span>
                <span class="n">objective</span><span class="p">,</span><span class="n">x0</span><span class="p">,</span><span class="n">jac</span><span class="o">=</span><span class="n">jacobian</span><span class="p">,</span><span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span><span class="n">constraints</span><span class="o">=</span><span class="n">con</span><span class="p">,</span>
                <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span>
                <span class="n">objective</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span><span class="n">jac</span><span class="o">=</span><span class="n">jacobian</span><span class="p">,</span> <span class="n">hess</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">,</span><span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
                <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">)</span>

        <span class="n">res</span><span class="p">[</span><span class="s1">&#39;obj_fun&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">objective</span>
        
        <span class="n">weights</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_full</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">res</span></div></div>

<div class="viewcode-block" id="optimal_experimental_design"><a class="viewcode-back" href="../../api/pyapprox.optimal_experimental_design.optimal_experimental_design.html#pyapprox.optimal_experimental_design.optimal_experimental_design">[docs]</a><span class="k">def</span> <span class="nf">optimal_experimental_design</span><span class="p">(</span><span class="n">design_pts</span><span class="p">,</span><span class="n">fun</span><span class="p">,</span><span class="n">criteria</span><span class="p">,</span><span class="n">regresion_type</span><span class="o">=</span><span class="s1">&#39;lstsq&#39;</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">solver_opts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pred_factors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cvar_tol</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute optimal experimental designs for models of the form</span>

<span class="sd">    .. math:: y(\rv)=m(\rv;\theta)+\eta(\rv)\epsilon</span>

<span class="sd">    to be used with estimators, such as least-squares and quantile regression, to find approximate parameters </span>
<span class="sd">    :math:`\hat{\theta}` that are the solutions of</span>
<span class="sd">    </span>
<span class="sd">    .. math:: \mathrm{argmin}_\theta \frac{1}{M}\sum_{i=1}^M e(y_i-m(\rv_i;\theta))</span>

<span class="sd">    for some loss function :math:`e`</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    design_pts : np.ndarray (nvars,nsamples)</span>
<span class="sd">        All possible experimental conditions</span>

<span class="sd">    design_factors : callable or np.ndarray</span>
<span class="sd">       The function :math:`m(\rv;\theta)` with the signature</span>
<span class="sd">    </span>
<span class="sd">       `design_factors(z,p)-&gt;np.ndarray`</span>
<span class="sd">    </span>
<span class="sd">       where `z` are the design points and `p` are the unknown</span>
<span class="sd">       parameters of the function which will be estimated</span>
<span class="sd">       from data collected using the optimal design</span>

<span class="sd">       A np.ndarray with shape (nsamples,nfactors) where</span>
<span class="sd">       each column is the jacobian of :math:`m(\rv,\theta)` for some :math:`\theta`</span>

<span class="sd">    criteria : string</span>
<span class="sd">       The optimality criteria. Supported criteria are</span>
<span class="sd">    </span>
<span class="sd">       - ``&#39;A&#39;``</span>
<span class="sd">       - ``&#39;D&#39;``</span>
<span class="sd">       - ``&#39;C&#39;``</span>
<span class="sd">       - ``&#39;I&#39;``</span>
<span class="sd">       - ``&#39;R&#39;``</span>
<span class="sd">       - ``&#39;G&#39;``</span>

<span class="sd">       The criteria I,G and R require pred_factors to be provided. A, C and D optimality do not. R optimality requires cvar_tol to be provided.</span>

<span class="sd">       See [KJLSIAMUQ2020]_ for a definition of these criteria</span>

<span class="sd">    regression_type : string</span>
<span class="sd">        The method used to compute the coefficients of the linear model. This defineds the loss function :math:`e`. </span>
<span class="sd">        Currently supported options are </span>

<span class="sd">        - ``&#39;lstsq&#39;`` </span>
<span class="sd">        - ``&#39;quantile&#39;``</span>

<span class="sd">        Both these options will produce the same design if noise_multiplier is None</span>

<span class="sd">    noise_multiplier : np.ndarray (nsamples)</span>
<span class="sd">        An array specifying the noise multiplier :math:`\eta` at each design point</span>

<span class="sd">    solver_opts : dict</span>
<span class="sd">        Options passed to the non-linear optimizer which solves</span>
<span class="sd">        the OED problem</span>

<span class="sd">    pred_factors : callable or np.ndarray</span>
<span class="sd">        The function :math:`g(\rv;\theta)` with the signature</span>
<span class="sd">    </span>
<span class="sd">        `design_factors(z,p)-&gt;np.ndarray`</span>
<span class="sd">    </span>
<span class="sd">        where `z` are the prediction points and `p` are the unknown</span>
<span class="sd">        parameters</span>

<span class="sd">        A np.ndarray with shape (nsamples,nfactors) where</span>
<span class="sd">       each column is the jacobian of :math:`g(\rv,\theta)` for some :math:`\theta`</span>

<span class="sd">    cvar_tol : float</span>
<span class="sd">        The :math:`0\le\beta&lt;1` quantile defining the R-optimality criteria. When :math:`\beta=0`, I and R optimal designs will be the same.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    final_design_pts : np.ndarray (nvars,nfinal_design_pts)</span>
<span class="sd">        The design points used in the experimental design</span>

<span class="sd">    nrepetitions : np.ndarray (nfinal_design_pts)</span>
<span class="sd">        The number of times to evaluate the model at each </span>
<span class="sd">        design point</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [KJLSIAMUQ2020] `D.P. Kouri, J.D. Jakeman, J. Lewis, Risk-Adapted Optimal Experimental Design.`</span>
<span class="sd">       </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">fun</span><span class="p">):</span>
        <span class="n">design_factors</span> <span class="o">=</span> <span class="n">fun</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">pred_factors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pred_factors&#39;</span><span class="p">:</span><span class="n">pred_factors</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">cvar_tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">opts</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">cvar_tol</span>
    
    <span class="n">ncandidate_design_pts</span> <span class="o">=</span> <span class="n">design_pts</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">opt_problem</span> <span class="o">=</span> <span class="n">AlphabetOptimalDesign</span><span class="p">(</span><span class="n">criteria</span><span class="p">,</span><span class="n">design_factors</span><span class="p">,</span><span class="n">regression_type</span><span class="o">=</span><span class="s1">&#39;quantile&#39;</span><span class="p">,</span><span class="n">noise_multiplier</span><span class="o">=</span><span class="n">noise_multiplier</span><span class="p">,</span><span class="n">opts</span><span class="o">=</span><span class="n">opts</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">solver_opts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">solver_opts</span>  <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;iprint&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;ftol&#39;</span><span class="p">:</span><span class="mf">1e-8</span><span class="p">}</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">opt_problem</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">solver_opts</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">mu</span><span class="o">*</span><span class="n">ncandidate_design_pts</span><span class="p">)</span>
    <span class="n">I</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mu</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">design_pts</span><span class="p">[:,</span><span class="n">I</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="n">I</span><span class="p">]</span></div>
    
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2019 National Technology &amp; Engineering Solutions of Sandia, LLC (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S. Government retains certain rights in this software.

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>