{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nApproximate Control Variate Monte Carlo\n=======================================\nThis tutorial builds upon `sphx_glr_auto_tutorials_multi_fidelity_plot_control_variate_monte_carlo.py` and describes how to implement and deploy *approximate* control variate Monte Carlo (ACVMC) sampling to compute expectations of model output from multiple low-fidelity models with unknown means. \n\nCVMC is often not useful for practical analysis of numerical models because typically the mean of the lower fidelity model, i.e. $\\mu_\\V{\\kappa}$, is unknown and the cost of the lower fidelity model is non trivial. These two issues can be overcome by using approximate control variate Monte Carlo.\n\nLet the cost of the high fidelity model per sample be $C_\\alpha$ and let the cost of the low fidelity model be $C_\\kappa$. Now lets use $N$ samples to estimate $Q_{\\V{\\alpha},N}$ and $Q_{\\V{\\kappa},N}$ and these  $N$ samples plus another $(r-1)N$ samples to estimate $\\mu_{\\V{\\kappa}}$ so that\n\n\\begin{align}Q_{\\V{\\alpha},N,r}^{\\text{ACV}}=Q_{\\V{\\alpha},N} + \\eta \\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa},N,r} \\right)\\end{align}\n\nand \n\n\\begin{align}\\mu_{\\V{\\kappa},N,r}=\\frac{1}{rN}\\sum_{i=1}^{rN}Q_\\V{\\kappa}\\end{align}\n\nWith this sampling scheme we have\n\n\\begin{align}Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa},N,r}&=\\frac{1}{N}\\sum_{i=1}^N f_\\V{\\kappa}^{(i)}-\\frac{1}{rN}\\sum_{i=1}^{rN}f_\\V{\\kappa}^{(i)}\\\\\n  &=\\frac{1}{N}\\sum_{i=1}^N f_\\V{\\kappa}^{(i)}-\\frac{1}{rN}\\sum_{i=1}^{N}f_\\V{\\kappa}^{(i)}-\\frac{1}{rN}\\sum_{i=N}^{rN}f_\\V{\\kappa}^{(i)}\\\\\n  &=\\frac{r-1}{rN}\\sum_{i=1}^N f_\\V{\\kappa}^{(i)}-\\frac{1}{rN}\\sum_{i=N}^{rN}f_\\V{\\kappa}^{(i)}\\\\\\end{align}\n\nwhere for ease of notation we write $r_\\V{\\kappa}N$ and $\\lfloor r_\\V{\\kappa}N\\rfloor$ interchangibly.\nUsing the above expression yields\n\n\\begin{align}\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa},N,r}\\right)}&=\\mean{\\left(\\frac{r-1}{rN}\\sum_{i=1}^N f_\\V{\\kappa}^{(i)}-\\frac{1}{rN}\\sum_{i=N}^{rN}f_\\V{\\kappa}^{(i)}\\right)^2}\\\\\n  &=\\frac{(r-1)^2}{r^2N^2}\\sum_{i=1}^N \\var{f_\\V{\\kappa}^{(i)}}+\\frac{1}{r^2N^2}\\sum_{i=N}^{rN}\\var{f_\\V{\\kappa}^{(i)}}\\\\\n  &=\\frac{(r-1)^2}{r^2N^2}N\\var{f_\\V{\\kappa}}+\\frac{1}{r^2N^2}(r-1)N\\var{f_\\V{\\kappa}}\\\\\n  %&=\\left(\\frac{(r-1)^2}{r^2N}+\\frac{(r-1)}{r^2N}\\right)\\var{f_\\V{\\kappa}}\\\\\n  &=\\frac{r-1}{r}\\frac{\\var{f_\\V{\\kappa}}}{N}\\end{align}\n\nwhere we have used the fact that since the samples used in the first and second term on the first line are not shared, the covariance between these terms is zero. Also we have\n\n\\begin{align}\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa},N,r}\\right)}=\\covar{\\frac{1}{N}\\sum_{i=1}^N f_\\V{\\alpha}^{(i)}}{\\frac{r-1}{rN}\\sum_{i=1}^N f_\\V{\\kappa}^{(i)}-\\frac{1}{rN}\\sum_{i=N}^{rN}f_\\V{\\kappa}^{(i)}}\\end{align}\n\nThe correlation between the estimators $\\frac{1}{N}\\sum_{i=1}^{N}Q_\\V{\\alpha}$ and $\\frac{1}{rN}\\sum_{i=N}^{rN}Q_\\V{\\kappa}$ is zero because the samples used in these estimators are different for each model. Thus\n\n\\begin{align}\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa},N,r}\\right)} &=\\covar{\\frac{1}{N}\\sum_{i=1}^N f_\\V{\\alpha}^{(i)}}{\\frac{r-1}{rN}\\sum_{i=1}^N f_\\V{\\kappa}^{(i)}}\\\\\n  &=\\frac{r-1}{r}\\frac{\\covar{f_\\V{\\alpha}}{f_\\V{\\kappa}}}{N}\\end{align}\n\nRecalling the variance reduction of the CV estimator using the optimal $\\eta$ is\n\n\\begin{align}\\gamma &= 1-\\frac{\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{ \\V{\\kappa},N,r}\\right)}^2}{\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa},N,r}\\right)}\\var{Q_{\\V{\\alpha},N}}}\\\\\n   &=1-\\frac{N^{-2}\\frac{(r-1)^2}{r^2}\\covar{f_\\V{\\alpha}}{f_\\V{\\kappa}}}{N^{-1}\\frac{r-1}{r}\\var{f_\\V{\\kappa}}N^{-1}\\var{f_\\V{\\alpha}}}\\\\\n   &=1-\\frac{r-1}{r}\\corr{f_\\V{\\alpha}}{f_\\V{\\kappa}}^2\\end{align}\n\nwhich is found when\n\n\\begin{align}\\eta&=-\\frac{\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa},N,r}\\right)}}{\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa},N,r}\\right)}}\\\\\n  &=-\\frac{N^{-1}\\frac{r-1}{r}\\covar{f_\\V{\\alpha}}{f_\\V{\\kappa}}}{N^{-1}\\frac{r-1}{r}\\var{f_\\V{\\kappa}}}\\\\\n  &=-\\frac{\\covar{f_\\V{\\alpha}}{f_\\V{\\kappa}}}{\\var{f_\\V{\\kappa}}}\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets setup the problem and compute an ACV estimate of $\\mean{f_0}$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pyapprox as pya\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pyapprox.tests.test_control_variate_monte_carlo import TunableModelEnsemble\nfrom scipy.stats import uniform\n\nnp.random.seed(1)\nshifts= [.1,.2]\nmodel = TunableModelEnsemble(1,shifts=shifts)\nexact_integral_f0=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before proceeding to estimate the mean using ACVMV we must first define how to generate samples to estimate $Q_{\\V{\\alpha},N}$ and $\\mu_{\\V{\\kappa},N,r}$. To do so clearly we must first introduce some additional notation. Let $\\mathcal{Z}_0$ be the set of samples used to evaluate the high-fidelity model and let $\\mathcal{Z}_\\alpha=\\mathcal{Z}_{\\alpha,1}\\cup\\mathcal{Z}_{\\alpha,2}$ be the samples used to evaluate the low fidelity model. Using this notation we can rewrite the ACV estimator as\n\n\\begin{align}Q_{\\V{\\alpha},\\mathcal{Z}}^{\\text{ACV}}=Q_{\\V{\\alpha},\\mathcal{Z}_0} + \\eta \\left( Q_{\\V{\\kappa},\\mathcal{Z}_{\\alpha,1}} - \\mu_{\\V{\\kappa},\\mathcal{Z}_{\\alpha,2}} \\right)\\end{align}\n\nwhere $\\mathcal{Z}=\\bigcup_{\\alpha=0}^M Z_\\alpha$. The nature of these samples can be changed to produce different ACV estimators. Here we choose  $\\mathcal{Z}_{\\alpha,1}\\cap\\mathcal{Z}_{\\alpha,2}=\\emptyset$ and $\\mathcal{Z}_{\\alpha,1}=\\mathcal{Z_0}$. That is we use the set a common set of samples to compute the covariance between all the models and a second independent set to estimate the lower fidelity mean. The sample partitioning for $M$ models is  shown in the following Figure. We call this scheme the ACV IS sampling stratecy where IS indicates that the second sample set $\\mathcal{Z}_{\\alpha,2}$ assigned to each model are not shared.\n\n.. list-table::\n\n   * - .. _acv-is-sample-allocation:\n\n       .. figure:: ../../figures/acv_is.png\n          :width: 50%\n          :align: center\n\n          ACV IS sampling strategy\n\nThe following code generates samples according to this strategy\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nhf_samples = int(1e1)\nnsample_ratio = 10\nsamples_shared = model.generate_samples(nhf_samples)\nsamples_lf_only =  model.generate_samples(nhf_samples*nsample_ratio-nhf_samples)\nvalues0 = model.m0(samples_shared)\nvalues1_shared = model.m1(samples_shared)\nvalues1_lf_only = model.m1(samples_lf_only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets plot the samples assigned to each model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots()\nax.plot(samples_shared[0,:],samples_shared[1,:],'ro',ms=12,\n        label=r'$\\mathrm{Low\\ and\\  high\\  fidelity\\  models}$')\nax.plot(samples_lf_only[0,:],samples_lf_only[1,:],'ks',\n        label=r'$\\mathrm{Low\\  fidelity\\  model\\ only}$')\nax.set_xlabel(r'$z_1$')\nax.set_ylabel(r'$z_2$',rotation=0)\n_ = ax.legend(loc='upper left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The high-fidelity model is only evaluated on the red dots. Now lets use these samples to estimate the mean of $f_0$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cov = model.get_covariance_matrix()\ngamma = 1-(nsample_ratio-1)/nsample_ratio*cov[0,1]**2/(cov[0,0]*cov[1,1])\neta = -cov[0,1]/cov[1,1]\nprint(values1_shared.shape,values1_lf_only.shape)\nacv_mean = values0.mean()+eta*(values1_shared.mean()-np.concatenate(\n    [values1_shared[:,0],values1_lf_only[:,0]]).mean())\nprint('MC difference squared =',(values0.mean()-exact_integral_f0)**2)\nprint('ACVMC difference squared =',(acv_mean-exact_integral_f0)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note here we have arbitrarily set the number of high fidelity samples $N$ and the ratio $r$. In practice one should choose these in one of two ways: (i) for a fixed budget choose the free parameters to minimize the variance of the estimator; or (ii) choose the free parameters to achieve a desired MSE (variance) with the smallest computational cost. Note the cost of computing the two model ACV estimator is\n\n\\begin{align}C_\\mathrm{cv} = NC_\\alpha + r_\\V{\\kappa}NC_\\kappa\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets compute the variance reduction for different sample sizes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_acv_two_model_variance_reduction(nsample_ratios,functions):\n    M = len(nsample_ratios) # number of lower fidelity models\n    assert len(functions)==M+1\n    \n    ntrials=int(1e3)\n    means = np.empty((ntrials,2))\n    for ii in range(ntrials):\n        samples_shared = model.generate_samples(nhf_samples)\n        # length M\n        samples_lf_only =[\n            model.generate_samples(nhf_samples*r-nhf_samples)\n            for r in nsample_ratios]\n        values_lf_only  =  [\n            f(s) for f,s in zip(functions[1:],samples_lf_only)]\n        # length M+1\n        values_shared  = [f(samples_shared) for f in functions]\n        #cov_mc  = np.cov(values_shared,rowvar=False)\n        # compute mean using only hf data\n        hf_mean = values_shared[0].mean()\n        means[ii,0]= hf_mean\n        # compute ACV mean\n        gamma=1-(nsample_ratios[0]-1)/nsample_ratios[0]*cov[0,1]**2/(\n            cov[0,0]*cov[1,1])\n        eta = -cov[0,1]/cov[1,1]\n        means[ii,1]=hf_mean+eta*(values_shared[1].mean()-\n            np.concatenate([values_shared[1],values_lf_only[0]]).mean())\n\n    print(\"Theoretical ACV variance reduction\",\n          1-(nsample_ratios[0]-1)/nsample_ratios[0]*cov[0,1]**2/(\n              cov[0,0]*cov[1,1]))\n    print(\"Achieved ACV variance reduction\",\n         means[:,1].var(axis=0)/means[:,0].var(axis=0))\n    return means\n\nr1,r2=10,100\nprint(f'Two model: r={r1}')\nmeans1 = compute_acv_two_model_variance_reduction([r1],[model.m0,model.m1])\nprint(f'Three model: r={r2}')\nmeans2 = compute_acv_two_model_variance_reduction([r2],[model.m0,model.m1])\nprint(\"Theoretical CV variance reduction\",1-cov[0,1]**2/(cov[0,0]*cov[1,1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us also plot the distribution of these estimators\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ntrials = means1.shape[0]\nfig,ax = plt.subplots()\nax.hist(means1[:,0],bins=ntrials//100,density=True,alpha=0.5,\n        label=r'$Q_{0,N}$')\nax.hist(means1[:,1],bins=ntrials//100,density=True,alpha=0.5,\n        label=r'$Q_{0,N,%d}^\\mathrm{CV}$'%r1)\nax.hist(means2[:,1],bins=ntrials//100,density=True,alpha=0.5,\n        label=r'$Q_{0,N,%d}^\\mathrm{CV}$'%r2)\nax.axvline(x=0,c='k',label=r'$E[Q_0]$')\n_ = ax.legend(loc='upper left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a fixed number of high-fidelity evaluations $N$ the ACVMC variance reduction will converge to the CVMC variance reduction. Try changing $N$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n^^^^^^^^^^\n.. [GGEJJCP2020] `A generalized approximate control variate framework for multifidelity uncertainty quantification, Journal of Computational Physics, 408:109257, 2020. <https://doi.org/10.1016/j.jcp.2020.109257>`_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}