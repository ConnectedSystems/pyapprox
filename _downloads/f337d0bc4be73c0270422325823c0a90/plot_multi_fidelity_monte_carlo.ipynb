{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nMulti-fidelity Monte Carlo\n==========================\nThis tutorial builds on from `sphx_glr_auto_tutorials_multi_fidelity_plot_multi_level_monte_carlo.py` and `sphx_glr_auto_tutorials_multi_fidelity_plot_approximate_control_variate_monte_carlo.py` and introduces an approximate control variate estimator called Multi-fidelity Monte Carlo (MFMC). Unlike MLMC this method does not assume a strict ordering of models.\n\nMany Model MFMC\n---------------\n\nTo derive the MFMC estimator first recall the two model ACV estimator\n\n\\begin{align}Q_{0,\\mathcal{Z}}^\\mathrm{MF}=Q_{0,\\mathcal{Z}_{0}} + \\eta\\left(Q_{1,\\mathcal{Z}_{0}}-\\mu_{1,\\mathcal{Z}_{1}}\\right)\\end{align}\n\nThe MFMC estimator can be derived with the following recursive argument. Partition the samples assigned to each model such that\n$\\mathcal{Z}_\\alpha=\\mathcal{Z}_{\\alpha,1}\\cup\\mathcal{Z}_{\\alpha,2}$ and $\\mathcal{Z}_{\\alpha,1}\\cap\\mathcal{Z}_{\\alpha,2}=\\emptyset$. That is the samples at the next lowest fidelity model are the samples used at all previous levels plus an additional independent set, i.e. $\\mathcal{Z}_{\\alpha,1}=\\mathcal{Z}_{\\alpha-1}$. See `mfmc-sample-allocation`. Note the differences between this scheme and the MLMC scheme.\n\n.. list-table::\n\n   * - \n\n       .. figure:: ../../figures/mfmc.png\n          :width: 100%\n          :align: center\n\n          MFMC sampling strategy\n\n     -\n\n       .. figure:: ../../figures/mlmc.png\n          :width: 100%\n          :align: center\n\n          MLMC sampling strategy\n\nStarting from two models we introduce the next low fidelity model in a way that reduces the variance of the estimate $\\mu_{\\alpha}$, i.e.\n\n\\begin{align}Q_{0,\\mathcal{Z}}^\\mathrm{MF}&=Q_{0,\\mathcal{Z}_{0}} + \\eta_1\\left(Q_{1,\\mathcal{Z}_{1}}-\\left(\\mu_{1,\\mathcal{Z}_{1}}+\\eta_2\\left(Q_{2,\\mathcal{Z}_1}-\\mu_{2,\\mathcal{Z}_2}\\right)\\right)\\right)\\\\\n   &=Q_{0,\\mathcal{Z}_{0}} + \\eta_1\\left(Q_{1,\\mathcal{Z}_{1}}-\\mu_{1,\\mathcal{Z}_{1}}\\right)+\\eta_1\\eta_2\\left(Q_{2,\\mathcal{Z}_1}-\\mu_{2,\\mathcal{Z}_2}\\right)\\end{align}\n\nWe repeat this process for all low fidelity models to obtain\n\n\\begin{align}Q_{0,\\mathcal{Z}}^\\mathrm{MF}=Q_{0,\\mathcal{Z}_{0}} + \\sum_{\\alpha=1}^M\\eta_\\alpha\\left(Q_{\\alpha,\\mathcal{Z}_{\\alpha,1}}-\\mu_{\\alpha,\\mathcal{Z}_{\\alpha}}\\right)\\end{align}\n\nThe optimal control variate weights for the MFMC estimator, which minimize the variance of the estimator, are $\\eta=(\\eta_1,\\ldots,\\eta_M)^T$, where for $\\alpha=1\\ldots,M$\n\n\\begin{align}\\eta_\\alpha = -\\frac{\\covar{Q_0}{Q_\\alpha}}{\\var{Q_\\alpha}}\\end{align}\n\nWith this choice of weights the variance reduction obtained is given by\n\n\\begin{align}\\gamma = 1-\\rho_1^2\\left(\\frac{r_1-1}{r_1}+\\sum_{\\alpha=2}^M \\frac{r_\\alpha-r_{\\alpha-1}}{r_\\alpha r_{\\alpha-1}}\\frac{\\rho_\\alpha^2}{\\rho_1^2}\\right)\\end{align}\n\nLet us use MFMC to estimate the mean of our high-fidelity model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport pyapprox as pya\nfrom functools import partial\nfrom pyapprox.tests.test_control_variate_monte_carlo import \\\n    TunableModelEnsemble, ShortColumnModelEnsemble, PolynomialModelEnsemble\nnp.random.seed(1)\n\n\nshort_column_model = ShortColumnModelEnsemble()\nmodel_ensemble = pya.ModelEnsemble(\n    [short_column_model.m0,short_column_model.m1,short_column_model.m2])\n\ncosts = np.asarray([100, 50, 5])\ntarget_cost = int(1e4)\nidx = [0,1,2]\ncov = short_column_model.get_covariance_matrix()[np.ix_(idx,idx)]\n\n# define the sample allocation\nnhf_samples,nsample_ratios = pya.allocate_samples_mfmc(\n    cov, costs, target_cost)[:2]\n# generate sample sets\nsamples,values =pya.generate_samples_and_values_mfmc(\n    nhf_samples,nsample_ratios,model_ensemble,\n    short_column_model.generate_samples)\n# compute mean using only hf data\nhf_mean = values[0][0].mean()\n# compute mlmc control variate weights\neta = pya.get_mfmc_control_variate_weights(cov)\n# compute MLMC mean\nmfmc_mean = pya.compute_approximate_control_variate_mean_estimate(eta,values)\n\n# get the true mean of the high-fidelity model\ntrue_mean = short_column_model.get_means()[0]\nprint('MLMC error',abs(mfmc_mean-true_mean))\nprint('MC error',abs(hf_mean-true_mean))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimal Sample Allocation\n-------------------------\nSimilarly to MLMC, the optimal number of samples that minimize the variance of the MFMC estimator can be determined analytically (see [PWGSIAM2016]_). Recalling that $C_\\mathrm{tot}$ is the total budget then the optimal number of high fidelity samples is\n\n\\begin{align}N_0 = \\frac{C_\\mathrm{tot}}{\\V{w}^T\\V{r}}\\end{align}\n\nwhere $\\V{r}=[r_0,\\ldots,r_M]^T$ are the sample ratios defining the number of samples assigned to each level, i.e. $N_\\alpha=r_\\alpha N_0$. The sample ratios are\n\n\\begin{align}r_\\alpha=\\left(\\frac{w_0(\\rho^2_{0,\\alpha}-\\rho^2_{0,\\alpha+1})}{w_\\alpha(1-\\rho^2_{0,1})}\\right)^{\\frac{1}{2}}\\end{align}\n\nwhere $\\V{w}=[w_0,w_M]^T$ are the relative costs of each model, and $\\rho_{j,k}$ is the correlation between models $j$ and $k$.\n\nNow lets us compare MC with MFMC using optimal sample allocations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "poly_model = PolynomialModelEnsemble()\nmodel_ensemble = pya.ModelEnsemble(poly_model.models)\ncov = poly_model.get_covariance_matrix()\ntarget_costs = np.array([1e1,1e2,1e3,1e4],dtype=int)\ncosts = np.asarray([10**-ii for ii in range(cov.shape[0])])\nmodel_labels=[r'$f_0$',r'$f_1$',r'$f_2$',r'$f_3$',r'$f_4$']\nvariances, nsamples_history = [],[]\nnpilot_samples = 5\nestimators = [pya.MC,pya.MFMC]\nfor target_cost in target_costs:\n    for estimator in estimators:\n        est = estimator(cov,costs)\n        nhf_samples,nsample_ratios = est.allocate_samples(target_cost)[:2]\n        variances.append(est.get_variance(nhf_samples,nsample_ratios))\n        nsamples_history.append(est.get_nsamples(nhf_samples,nsample_ratios))\nvariances = np.asarray(variances)\nnsamples_history = np.asarray(nsamples_history)\n\nfig,axs=plt.subplots(1,2,figsize=(2*8,6))\n# plot sample allocation\npya.plot_acv_sample_allocation(nsamples_history[1::2],costs,model_labels,axs[1])\nmfmc_total_costs = np.array(nsamples_history[1::2]).dot(costs)\nmfmc_variances = variances[1::2]\naxs[0].loglog(mfmc_total_costs,mfmc_variances,':',label=r'$\\mathrm{MFMC}$')\nmc_total_costs = np.array(nsamples_history[::2]).dot(costs)\nmc_variances = variances[::2]\naxs[0].loglog(mc_total_costs,mc_variances,label=r'$\\mathrm{MC}$')\naxs[0].set_ylim(axs[0].get_ylim()[0],1e-2)\n_ = axs[0].legend()\n#plt.show()\n#fig # necessary for jupyter notebook to reshow plot in new cell"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}