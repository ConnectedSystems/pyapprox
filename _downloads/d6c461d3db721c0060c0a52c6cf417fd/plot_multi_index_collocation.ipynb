{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nMulti-index Stochastic Collocation\n==================================\nThis tutorial describes how to implement and deploy multi-index collocation to construct a surrogate of the output of a high-fidelity model using a set of lower-fidelity models of lower accuracy and cost.\n\nDespite the improved efficiency of surrogate methods relative to MC sampling, building a surrogate can still be prohibitively expensive for high-fidelity simulation models. Fortunately, a selection of models of varying fidelity and computational cost are typically available for many applications. For example, aerospace models span fluid dynamics, structural and thermal response, control systems, etc. \n\nLeveraging an ensemble of models can facilitate significant reductions in the overall computational cost of UQ, by integrating the predictions of quantities of interest (QoI) from multiple sources.\n\n\\begin{align}\\frac{\\partial u}{\\partial t}(x,t,\\rv) + \\nabla u(x,t,\\rv)-\\nabla\\cdot\\left[k(x,\\rv) \\nabla u(x,t,\\rv)\\right] &= g(x,t) \\qquad\\qquad (x,t,\\rv)\\in D\\times [0,1]\\times\\rvdom\\\\\n   u(x,t,\\rv)&=0 \\qquad\\qquad\\qquad (x,t,\\rv)\\in \\partial D\\times[0,1]\\times\\rvdom\\end{align}\n\nwith forcing $g(x,t)=(1.5+\\cos(2\\pi t))\\cos(x_1)$, and subject to the initial condition $u(x,0,\\rv)=0$. Following [NTWSIAMNA2008]_, we model the diffusivity $k$ as a random field represented by the\nKarhunen-Loeve (like) expansion (KLE)\n\n\\begin{align}\\log(k(x,\\rv)-0.5)=1+\\rv_1\\left(\\frac{\\sqrt{\\pi L}}{2}\\right)^{1/2}+\\sum_{k=2}^d \\lambda_k\\phi(x)\\rv_k,\\end{align}\n\nwith\n\n\\begin{align}\\lambda_k=\\left(\\sqrt{\\pi L}\\right)^{1/2}\\exp\\left(-\\frac{(\\lfloor\\frac{k}{2}\\rfloor\\pi L)^2}{4}\\right) k>1,  \\qquad\\qquad  \\phi(x)=\n    \\begin{cases}\n      \\sin\\left(\\frac{(\\lfloor\\frac{k}{2}\\rfloor\\pi x_1)}{L_p}\\right) & k \\text{ even}\\,,\\\\\n      \\cos\\left(\\frac{(\\lfloor\\frac{k}{2}\\rfloor\\pi x_1)}{L_p}\\right) & k \\text{ odd}\\,.\n    \\end{cases}\\end{align}\n\nwhere $L_p=\\max(1,2L_c)$, $L=\\frac{L_c}{L_p}$ and $L_c=0.5$.\n\nWe choose a random field which is effectively one-dimensional so that the error in the finite element solution is more sensitive to refinement of the mesh in the $x_1$-direction than to refinement in the $x_2$-direction.\n\nThe advection diffusion equation is solved using linear finite elements and implicit backward-Euler timestepping implemented using `Fenics <https://fenicsproject.org/>`_. In the following we will show how solving the PDE with varying numbers of finite elements and timesteps can reduce the cost of approximating the quantity of interest\n\n\\begin{align}f(\\rv)=\\int_D u(\\rv)\\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{\\lVert x-x^\\star \\rVert_2^2}{\\sigma^2}\\right)\\,dx,\\end{align}\n\nwhere $x^\\star=(0.3,0.5)$ and $\\sigma=0.16$.\n\nLets first consider a simple example with one unknown parameter. The following sets up the problem\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pyapprox as pya\nfrom scipy.stats import uniform\nfrom pyapprox.models.wrappers import MultiLevelWrapper\nimport matplotlib.pyplot as plt\n\nnmodels  = 3\nnrandom_vars, corr_len = 1, 1/2\nmax_eval_concurrency = 1\nfrom pyapprox.benchmarks.benchmarks import setup_benchmark\nbenchmark = setup_benchmark(\n    'multi_level_advection_diffusion',nvars=nrandom_vars,corr_len=corr_len,\n    max_eval_concurrency=max_eval_concurrency)\nmodel = benchmark.fun\nvariable = benchmark.variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets us plot each model as a function of the random variable\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lb,ub = variable.get_statistics('interval',alpha=1)[0]\nnsamples = 10\nrandom_samples = np.linspace(lb,ub,nsamples)[np.newaxis,:]\nconfig_vars = np.arange(nmodels)[np.newaxis,:]\nsamples = pya.get_all_sample_combinations(random_samples,config_vars)\nvalues = model(samples)\nvalues = np.reshape(values,(nsamples,nmodels))\n\nimport dolfin as dl\nplt.figure(figsize=(nmodels*8,2*6))\nconfig_samples = benchmark['multi_level_model'].map_to_multidimensional_index(config_vars)\nfor ii in range(nmodels):\n    nx,ny = model.base_model.get_mesh_resolution(config_samples[:2,ii])\n    dt = model.base_model.get_timestep(config_samples[2,ii])\n    mesh = dl.RectangleMesh(dl.Point(0, 0),dl.Point(1, 1), nx, ny)\n    plt.subplot(2,nmodels,ii+1)\n    dl.plot(mesh)\n    label=r'$f_%d$'%ii\n    if ii==0:\n        ax = plt.subplot(2,nmodels,nmodels+ii+1)\n    else:\n        plt.subplot(2,nmodels,nmodels+ii+1,sharey=ax)\n    plt.plot(random_samples[0,:],values[:,ii],label=label)\n    if ii>0:\n        label=r'$f_%d-f_%d$'%(ii,ii-1)\n        plt.plot(random_samples[0,:],values[:,ii]-values[:,ii-1],label=label)\n    plt.legend()\n#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first row shows the spatial mesh of each model and the second row depicts the model response and the discrepancy between two consecutive models. The difference between the model output decreases as the resolution of the mesh is increased. Thus as the cost of the model increases (with increasing resolution) we need less samples to resolve\n\nLets now construct a multi-level approximation with the same model but more random variables. We will need a model that takes in only 1 configuration variable. We can do this with the :class:`MultiLevelWrapper`. Here we will call another benchmark which is the same advection diffusion problem but with the multi-level wrapper already in place. Here the levels have the one-to-one mapping [0,1,2,...]->[[0,0,0],[1,1,1],[2,2,2],...]\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nrandom_vars = 10\nbenchmark = setup_benchmark(\n    'multi_level_advection_diffusion',nvars=nrandom_vars,corr_len=corr_len,\n    max_eval_concurrency=max_eval_concurrency)\nmodel = benchmark.fun\nvariable = benchmark.variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First define the levels of the multi-level model we will use. Will will skip level 0 and use levels 1,2, and 3. Thus we must define a transformation that converts the sparse grid indices starting at 0 to these levels. We can do this with\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pyapprox.adaptive_sparse_grid import ConfigureVariableTransformation\nlevel_indices = [[1,2,3,4]]\nconfig_var_trans = ConfigureVariableTransformation(level_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before building the sparse grid approximation let us define a callback to compute the error and total cost at each step of the sparse grid construction. To do this we will precompute some validation data. Specifically we will evaluate the model using a discretization on level higher than the discretization used to construct the sparse grid. We first generate random samples and then append in the configure variable to each of these samples\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "validation_level = level_indices[0][-1]+1\nnvalidation_samples = 20\nrandom_validation_samples = pya.generate_independent_random_samples(variable,nvalidation_samples)\nvalidation_samples = np.vstack([random_validation_samples,validation_level*np.ones((1,nvalidation_samples))])\nvalidation_values = model(validation_samples)\n\n#print(model.work_tracker.costs)\n\nerrors,total_cost = [],[]\ndef callback(approx):\n    approx_values=approx.evaluate_using_all_data(\n        validation_samples)\n    error = np.linalg.norm(\n        validation_values-approx_values)/np.sqrt(validation_samples.shape[1])\n    errors.append(error)\n    total_cost.append(approx.num_equivalent_function_evaluations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can add this callback to the sparse grid options. We define ``max_nsamples``\nto be the total cost used to evaluate all samples in the sparse grid\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_nsamples = 100\nfrom pyapprox.multifidelity import adaptive_approximate_multi_index_sparse_grid\ncost_function = model.cost_function\ndef cost_function(multilevel_config_sample):\n    config_sample = benchmark.multi_level_model.map_to_multidimensional_index(\n        multilevel_config_sample)\n    nx,ny=model.base_model.get_mesh_resolution(config_sample[:2])\n    dt = model.base_model.get_timestep(config_sample[2])\n    ndofs = nx*ny*model.base_model.final_time/dt\n    return ndofs/1e5\n\noptions = {'config_var_trans':config_var_trans,'max_nsamples':max_nsamples,\n           'config_variables_idx':nrandom_vars,'verbose':0,\n           'cost_function':cost_function,\n           'max_level_1d':[np.inf]*nrandom_vars+[\n               len(level_indices[0])-1]*len(level_indices),\n           'callback':callback}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets us build the sparse grid\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sparse_grid = adaptive_approximate_multi_index_sparse_grid(\n    model,variable.all_variables(),options)\n\n# #%%\n# #Lets plot the errors\n# fig, ax = plt.subplots(1,1,figsize=(8,6))\n# plt.loglog(total_cost,errors)\n# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets build a single fidelity approximation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "multi_level_indices = level_indices.copy()\nlevel_indices = [[level_indices[0][-1]]]\nconfig_var_trans = ConfigureVariableTransformation(level_indices)\n#cost_function = model.cost_function\noptions = {'config_var_trans':config_var_trans,'max_nsamples':max_nsamples,\n           'config_variables_idx':nrandom_vars,'verbose':0,\n           'cost_function':cost_function,\n           'max_level_1d':[np.inf]*nrandom_vars+[\n               len(level_indices[0])-1]*len(level_indices),\n           'callback':callback}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets us build the sparse grid. First reset the callback counters and save the previous errors\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "multi_level_errors,multi_level_total_cost = errors.copy(), total_cost.copy()\nerrors,total_cost = [],[]\nsparse_grid = adaptive_approximate_multi_index_sparse_grid(\n    model,variable.all_variables(),options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets plot the errors\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(8,6))\nplt.loglog(multi_level_total_cost,multi_level_errors,\n           label=r'$\\mathrm{Multi}-\\mathrm{level}$')\nplt.loglog(total_cost,errors,\n           label=r'$\\mathrm{Single}-\\mathrm{Fidelity}$')\nplt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets build a multi-index approximation of the same model but now with more random variables. We again create the same benchmark but this time one that allows us to vary the two spatial mesh resolutions and the timestep independently\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmark = setup_benchmark(\n    'multi_index_advection_diffusion',nvars=nrandom_vars,corr_len=corr_len,\n    max_eval_concurrency=max_eval_concurrency)\nmodel = benchmark.fun\nvariable = benchmark.variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again we define the ConfigureVariableTransformation and define the appropriate options. Notice the different in max_level_1d. (print it; it should be longer than the single fidelity and multi-level versions)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pyapprox.adaptive_sparse_grid import ConfigureVariableTransformation\nlevel_indices = multi_level_indices*3\nconfig_var_trans = ConfigureVariableTransformation(level_indices)\n\nfrom pyapprox.multifidelity import adaptive_approximate_multi_index_sparse_grid\n#cost_function = model.cost_function\ndef cost_function(config_sample):\n    nx,ny=model.base_model.get_mesh_resolution(config_sample[:2])\n    dt = model.base_model.get_timestep(config_sample[2])\n    ndofs = nx*ny*model.base_model.final_time/dt\n    return ndofs/1e5\noptions = {'config_var_trans':config_var_trans,'max_nsamples':max_nsamples,\n           'config_variables_idx':nrandom_vars,'verbose':0,\n           'cost_function':cost_function,\n           'max_level_1d':[np.inf]*nrandom_vars+[\n               len(level_indices[0])-1]*len(level_indices),\n           'callback':callback}\n\nsingle_fidelity_errors,single_fidelity_total_cost = errors.copy(), total_cost.copy()\nerrors,total_cost = [],[]\nsparse_grid = adaptive_approximate_multi_index_sparse_grid(\n    model,variable.all_variables(),options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets plot the errors\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(8,6))\nplt.loglog(multi_level_total_cost,multi_level_errors,\n           label=r'$\\mathrm{Multi}-\\mathrm{level}$')\nplt.loglog(single_fidelity_total_cost,single_fidelity_errors,\n           label=r'$\\mathrm{Single}-\\mathrm{Fidelity}$')\nplt.loglog(total_cost,errors,label=r'$\\mathrm{Multi}-\\mathrm{index}$')\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n^^^^^^^^^^\n.. [TJWGSIAMUQ2015] `Teckentrup, A. and Jantsch, P. and Webster, C. and Gunzburger, M. A Multilevel Stochastic Collocation Method for Partial Differential Equations with Random Input Data. SIAM/ASA Journal on Uncertainty Quantification, 3(1), 1046-1074, 2015. <https://doi.org/10.1137/140969002>`_\n\n.. [HNTTCMAME2016] `Haji-Ali, A. and Nobile, F. and Tamellini, L. and Tempone, R. Multi-Index Stochastic Collocation for random PDEs. Computer Methods in Applied Mechanics and Engineering, 306, 95-122, 2016. <https://doi.org/10.1016/j.cma.2016.03.029>`_\n\n.. [JEGGIJNME2020] `Jakeman, J.D., Eldred, M.S., Geraci, G., Gorodetsky, A. Adaptive multi-index collocation for uncertainty quantification and sensitivity analysis. Int J Numer Methods Eng. 2020; 121: 1314\u2013 1343. <https://doi.org/10.1002/nme.6268>`_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}