{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nMulti-level Monte Carlo\n=======================\nThis tutorial builds upon `sphx_glr_auto_tutorials_multi_fidelity_plot_approximate_control_variate_monte_carlo.py` and describes how the pioneering work of Multi-level Monte Carlo [CGSTCVS2011]_, [GOR2008]_ can be used to estimate the mean of a high-fidelity model using multiple low fidelity models. \n\nTwo Model MLMC\n--------------\nThe two model MLMC estimator is very similar to the CVMC estimator and is based upon the observations that we can estimate\n\n\\begin{align}\\mean{f_0}=\\mean{f_1}+\\mean{f_0-f_1}\\end{align}\n\nusing the following unbiased estimator\n\n\\begin{align}Q_{0,\\mathcal{Z}}^\\mathrm{ML}=N_1^{-1}\\sum_{i=1}^{N_1} f_{1,\\mathcal{Z}_1}^{(i)}+N_0^{-1}\\sum_{i=1}^{N_0} \\left(f_{0,\\mathcal{Z}_0}^{(i)}-f_{1,\\mathcal{Z}_0}^{(i)}\\right)\\end{align}\n\nWhere samples $\\mathcal{Z}=\\mathcal{Z}_0\\bigcup\\mathcal{Z}_1$ with $\\mathcal{Z}_0\\bigcap\\mathcal{Z}_1=\\emptyset$ and $f_{\\alpha,\\mathcal{Z}_\\kappa}^{(i)}$ denotes an evaluation of $f_\\alpha$ using a sample from the set $\\mathcal{Z}_\\kappa$. More simply to evaluate this estimator we must evaluate the low fidelity model at a set of samples  $\\mathcal{Z}_1$ and then evaluate the difference between the two models at another independet set $\\mathcal{Z}_0$.\n\nTo simplify notation let\n\n\\begin{align}Y_{\\alpha,\\mathcal{Z}_{\\alpha-1}}=-N_{\\alpha}^{-1}\\sum_{i=1}^{N_{\\alpha}} \\left(f_{\\alpha+1,\\mathcal{Z}_{\\alpha}}^{(i)}-f_{\\alpha,\\mathcal{Z}_{\\alpha}}^{(i)}\\right)\\end{align}\n\nso we can write \n\n\\begin{align}Q_{0,\\mathcal{Z}}^\\mathrm{ML}=Y_{0,\\mathcal{Z}_0}+Y_{1,\\mathcal{Z}_1}\\end{align}\n\nwhere $f_{2}=0$.\nThe variance of this estimator is given by\n\n\\begin{align}\\var{Q_{0,\\mathcal{Z}}^\\mathrm{ML}}=\\var{Y_{0,\\mathcal{Z}_0}+Y_{1,\\mathcal{Z}_1}}=N_0^{-1}\\var{Y_{0,\\mathcal{Z}_0}}+N_1^{-1}\\var{Y_{1,\\mathcal{Z}_1}}\\end{align}\n  \nsince $\\mathcal{Z}_0\\bigcap\\mathcal{Z}_1=\\emptyset.$\n\nFrom the previous equation we can see that MLMC works well if the variance of the difference between models is smaller than the variance of either model. Although the variance of the low-fidelity model is likely high, we can set $N_0$ to be large because the cost of evaluating the low-fidelity model is low. If the variance of the discrepancy is smaller we only need a smaller number of samples so that the two sources of error (the two terms on the RHS of the previous equation) are balanced.\n\nNote above and below we assume that $f_0$ is the high-fidelity model, but typical multi-level literature assumes $f_M$ is the high-fidelity model. We adopt the reverse ordering to be consistent with control variate estimator literature.\n\nMany Model MLMC\n---------------\n\nMLMC can easily be extended to estimator based on $M+1$ models. Letting $f_0,\\ldots,f_M$ be an ensemble of $M+1$ models ordered by decreasing fidelity and cost (note typically MLMC literature reverses this order),  we simply introduce estimates of the differences between the remaining models. That is\n\n.. math :: \n   :label: eq:ml-estimator\n\n   Q_{0,\\mathcal{Z}}^\\mathrm{ML} = \\sum_{\\alpha=0}^M Y_{\\alpha,\\mathcal{Z}_\\alpha}, \\quad f_{M+1}=0\n\nTo compute this estimator we use the following algorithm, tarting with $\\alpha=M$$\n\n#. Draw  $N_\\alpha$ samples randomly from the PDF  $\\pdf$ of the random variables.\n#. Estimate $f_{\\alpha}$ and $f_{\\alpha}$ at the samples $\\mathcal{Z}_\\alpha$\n#. Compute the discrepancies $Y_{\\alpha,\\mathcal{Z}_\\alpha}$ at each sample.\n#. Decrease $\\alpha$ and repeat steps 1-3. until $\\alpha=0$.\n#. Compute the ML estimator using :eq:`eq:ml-estimator`\n\nThe sampling strategy used for MLMC is shown in figure `mlmc-sample-allocation`. Here it is clear that we evaluate only the lowest fidelity model with $\\mathcal{Z}_M$ (this follows from the assumption that $f_{M+1}=0$) and to to evaluate each discrepancy we must evaluate two consecutive models at the sets $\\mathcal{Z}_\\alpha$.\n\n.. list-table::\n\n   * - \n\n       .. figure:: ../../figures/mlmc.png\n          :width: 50%\n          :align: center\n\n          MLMC sampling strategy\n\nBecause we use independent samples to estimate the expectation of each discrepancy, the variance of the estimator is\n\n\\begin{align}\\var{Q_{0,\\mathcal{Z}}^\\mathrm{ML}} = \\sum_{\\alpha=0}^M N_\\alpha\\var{Y_{\\alpha,\\mathcal{Z}_\\alpha}}\\end{align}\n\n\nLets setup a problem to compute an MLMC estimate of $\\mean{f_0}$\nusing the following ensemble of models, taken from [PWGSIAM2016]_, which simulate a short column with a rectangular cross-sectional area subject to bending and axial force.\n\n\\begin{align}f_0(z)&=(1 - 4M/(b(h^2)Y) - (P/(bhY))^2)\\\\\n   f_1(z)&=(1 - 3.8M/(b(h^2)Y) - ((P(1 + (M-2000)/4000))/(bhY))^2)\\\\\n   f_2(z)&=(1 - M/(b(h^2)Y) - (P/(bhY))^2)\\\\\n   f_3(z)&=(1 - M/(b(h^2)Y) - (P(1 + M)/(bhY))^2)\\\\\n   f_4(z)&=(1 - M/(b(h^2)Y) - (P(1 + M)/(hY))^2)\\end{align}\n\nwhere $z = (b,h,P,M,Y)^T$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pyapprox as pya\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pyapprox.tests.test_control_variate_monte_carlo import \\\n    TunableModelEnsemble, ShortColumnModelEnsemble, PolynomialModelEnsemble \nfrom scipy.stats import uniform\nfrom functools import partial\nfrom scipy.stats import uniform,norm,lognorm\nnp.random.seed(1)\n\nshort_column_model = ShortColumnModelEnsemble()\nmodel_ensemble = pya.ModelEnsemble(\n    [short_column_model.m0,short_column_model.m1,short_column_model.m2])\n\ncosts = np.asarray([100, 50, 5])\ntarget_cost = int(1e4)\nidx = [0,1,2]\ncov = short_column_model.get_covariance_matrix()[np.ix_(idx,idx)]\n# generate pilot samples to estimate correlation\n# npilot_samples = int(1e4)\n# cov = pya.estimate_model_ensemble_covariance(\n#    npilot_samples,short_column_model.generate_samples,model_ensemble)[0]\n\n# define the sample allocation\nnhf_samples,nsample_ratios = pya.allocate_samples_mlmc(\n    cov, costs, target_cost)[:2]\n# generate sample sets\nsamples,values =pya.generate_samples_and_values_mlmc(\n    nhf_samples,nsample_ratios,model_ensemble,\n    short_column_model.generate_samples)\n# compute mean using only hf data\nhf_mean = values[0][0].mean()\n# compute mlmc control variate weights\neta = pya.get_mlmc_control_variate_weights(cov.shape[0])\n# compute MLMC mean\nmlmc_mean = pya.compute_approximate_control_variate_mean_estimate(eta,values)\n\n# get the true mean of the high-fidelity model\ntrue_mean = short_column_model.get_means()[0]\nprint('MLMC error',abs(mlmc_mean-true_mean))\nprint('MC error',abs(hf_mean-true_mean))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These errors are comparable. However these errors are only for one realization of the samples sets. To obtain a clearer picture on the benefits of MLMC we need to look at the variance of the estimator.\n\nBy viewing MLMC as a control variate we can derive its variance reduction [GGEJJCP2020]_\n\n\\begin{align}\\gamma+1 = - \\eta_1^2 \\tau_{1}^2 - 2 \\eta_1 \\rho_{1} \\tau_{1} - \\eta_M^2 \\frac{\\tau_{M}}{\\hat{r}_{M}} - \\sum_{i=2}^M \\frac{1}{\\hat{r}_{i-1}}\\left( \\eta_i^2 \\tau_{i}^2 + \\tau_{i-1}^2 \\tau_{i-1}^2 - 2 \\eta_i \\eta_{i-1} \\rho_{i,i-1} \\tau_{i} \\tau_{i-1} \\right),\n   :label: mlmc-variance-reduction\\end{align}\n\nwhere  $\\tau_\\alpha=\\left(\\frac{\\var{Q_\\alpha}}{\\var{Q_0}}\\right)^{\\frac{1}{2}}$. Recall that and $\\hat{r}_\\alpha=\\lvert\\mathcal{Z}_{\\alpha,2}\\rvert/N$ is the ratio of the cardinality of the sets $\\mathcal{Z}_{\\alpha,2}$ and $\\mathcal{Z}_{0,2}$. \n\nThe following code computes the variance reduction of the MLMC estimator, using the 2 models $f_0,f_1$. The variance reduction is estimated numerically by  running MLMC repeatedly with different realizations of the sample sets. The function ``get_rsquared_mlmc`` is used to return the theoretical variance reduction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ntrials=1e1\nget_cv_weights_mlmc = pya.get_mlmc_control_variate_weights_pool_wrapper\nmeans1, numerical_var_reduction1, true_var_reduction1 = \\\n    pya.estimate_variance_reduction(\n        model_ensemble, cov[:2,:2], short_column_model.generate_samples,\n        pya.allocate_samples_mlmc,\n        pya.generate_samples_and_values_mlmc, get_cv_weights_mlmc,\n        pya.get_rsquared_mlmc,ntrials=ntrials,max_eval_concurrency=1,\n        costs=costs[:2],target_cost=target_cost)\nprint(\"Theoretical 2 model MLMC variance reduction\",true_var_reduction1)\nprint(\"Achieved 2 model MLMC variance reduction\",numerical_var_reduction1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The numerical estimate of the variance reduction is consistent with the theory.\nNow let us compute the theoretical variance reduction using 3 models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "true_var_reduction2 = 1-pya.get_rsquared_mlmc(cov,nsample_ratios)\nprint(\"Theoretical 3 model MLMC variance reduction\",true_var_reduction2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variance reduction obtained using three models is only slightly better than when using two models. The difference in variance reduction is dependent on the correlations between the models and the number of samples assigned to each model.\n\nMLMC works extremely well when the variance between the model discrepancies decays with increaseing fidelity. However one needs to be careful that the variance between discrepancies does indeed decay. Sometimes when the models correspond to different mesh discretizations. Increasing the mesh resolution does not always produce a smaller discrepancy. The following example shows that, for this example, adding a third model actually increases the variance of the MLMC estimator. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_ensemble = pya.ModelEnsemble(\n    [short_column_model.m0,short_column_model.m3,short_column_model.m4])\nidx = [0,3,4]\ncov3 = short_column_model.get_covariance_matrix()[np.ix_(idx,idx)]\ntrue_var_reduction3 = 1-pya.get_rsquared_mlmc(cov3,nsample_ratios)\nprint(\"Theoretical 3 model MLMC variance reduction for a pathalogical example\",true_var_reduction3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using MLMC for this ensemble of models creates an estimate with a variance orders of magnitude larger than just using the high-fidelity model. When using models that do not admit a hierarchical structure, alternative approaches are needed. We will introduce such estimators in future tutorials.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimal Sample Allocation\n-------------------------\nUp to this point we have assumed that the number of samples allocated to each model was fixed. However one major advantage of MLMC is that we can analytically determine the optimal number of samples assigned to each model used in a MLMC estimator. This section follows closely the exposition in [GAN2015]_.\n\nLet $C_\\alpha$ be the cost of evaluating the function $f_\\alpha$ at a single sample, then the total cost of the MLMC estimator is\n\n\\begin{align}C_{\\mathrm{tot}}=\\sum_{l=0}^M C_\\alpha N_\\alpha\\end{align}\n\nNow recall that the variance of the estimator is\n\n\\begin{align}\\var{Q_0^\\mathrm{ML}}=\\sum_{\\alpha=0}^M \\var{Y_\\alpha}N_\\alpha,\\end{align}\n\nwhere $Y_\\alpha$ is the disrepancy between two consecutive models, e.g. $f_{\\alpha-1}-f_\\alpha$ and $N_\\alpha$ be the number of samples allocated to resolving the discrepancy, i.e. $N_\\alpha=\\lvert\\hat{\\mathcal{Z}}_\\alpha\\rvert$. Then For a fixed variance $\\epsilon^2$ the cost of the MLMC estimator can be minimized, by solving\n\n\\begin{align}\\min_{N_0,\\ldots,N_M} & \\sum_{\\alpha=0}^M\\left(N_\\alpha C_\\alpha\\right)\\\\\n  \\mathrm{subject}\\; \\mathrm{to} &\\sum_{\\alpha=0}^M\\left(N_\\alpha^{-1}\\var{Y_\\alpha}\\right)=\\epsilon^2\\end{align}\n\nor alternatively by introducing the lagrange multiplier $\\lambda^2$ we can minimize\n\n\\begin{align}\\mathcal{J}(N_0,\\ldots,N_M,\\lambda)&=\\sum_{\\alpha=0}^M\\left(N_\\alpha C_\\alpha\\right)+\\lambda^2\\left(\\sum_{\\alpha=0}^M\\left(N_\\alpha^{-1}\\var{Y_\\alpha}\\right)-\\epsilon^2\\right)\\\\\n   &=\\sum_{\\alpha=0}^M\\left(N_\\alpha C_\\alpha+\\lambda^2N_\\alpha^{-1}\\var{Y_\\alpha}\\right)-\\lambda^2\\epsilon^2\\end{align}\n\nTo find the minimum we set the gradient of this expression to zero:\n\n\\begin{align}\\frac{\\partial \\mathcal{J}^\\mathrm{ML}}{N_\\alpha}&=C_\\alpha-\\lambda^2N_\\alpha^{-2}\\var{Y_\\alpha}=0\\\\\n  \\implies C_\\alpha&=\\lambda^2N_\\alpha^{-2}\\var{Y_\\alpha}\\\\\n  \\implies N_\\alpha&=\\lambda\\sqrt{\\var{Y_\\alpha}C_\\alpha^{-1}}\\end{align}\n\nThe constraint is satisifed by noting \n\n\\begin{align}\\frac{\\partial \\mathcal{J}}{\\lambda^2}=\\sum_{\\alpha=0}^M N_\\alpha^{-1}\\var{Y_\\alpha}-\\epsilon^2=0\\end{align}\n\nRecalling that we can write the total variance as\n\n\\begin{align}\\var{Q_{0,\\mathcal{Z}}^\\mathrm{ML}}&=\\sum_{\\alpha=0}^M N_\\alpha^{-1} \\var{Y_\\alpha}\\\\\n  &=\\sum_{\\alpha=0}^M \\lambda^{-1}\\var{Y_\\alpha}^{-\\frac{1}{2}}C_\\alpha^{\\frac{1}{2}}\\var{Y_\\alpha}\\\\\n  &=\\lambda^{-1}\\sum_{\\alpha=0}^M\\sqrt{\\var{Y_\\alpha}C_\\alpha}=\\epsilon^2\\\\\n  \\implies \\lambda &= \\epsilon^{-2}\\sum_{\\alpha=0}^M\\sqrt{\\var{Y_\\alpha}C_\\alpha}\\end{align}\n\nThen substituting $\\lambda$ into the following\n\n\\begin{align}N_\\alpha C_\\alpha&=\\lambda\\sqrt{\\var{Y_\\alpha}C_\\alpha^{-1}}C_\\alpha\\\\\n  &=\\lambda\\sqrt{\\var{Y_\\alpha}C_\\alpha}\\\\\n  &=\\epsilon^{-2}\\left(\\sum_{\\alpha=0}^M\\sqrt{\\var{Y_\\alpha}C_\\alpha}\\right)\\sqrt{\\var{Y_\\alpha}C_\\alpha}\\end{align}\n\nallows us to determine the smallest total cost that generates and estimator with the desired variance.\n\n\\begin{align}C_\\mathrm{tot}&=\\sum_{\\alpha=0}^M N_\\alpha C_\\alpha\\\\\n  &=\\sum_{\\alpha=0}^M \\epsilon^{-2}\\left(\\sum_{\\alpha=0}^M\\sqrt{\\var{Y_\\alpha}C_\\alpha}\\right)\\sqrt{\\var{Y_\\alpha}C_\\alpha}\\\\\n  &=\\epsilon^{-2}\\left(\\sum_{\\alpha=0}^M\\sqrt{\\var{Y_\\alpha}C_\\alpha}\\right)^2\\end{align}\n\nTo demonstrate the effectiveness of MLMC consider the model ensemble\n\n\\begin{align}f_\\alpha(\\rv)=\\rv^{5-\\alpha}, \\quad \\alpha=0,\\ldots,4\\end{align}\n\nwhere each model is the function of a single uniform random variable defined on the unit interval $[0,1]$. The following code computes the variance of the MLMC estimator for different target costs using the optimal sample allocation using an exact estimate of the covariance between models and an approximation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pyapprox as pya\nimport matplotlib.pyplot as plt\nfrom pyapprox.tests.test_control_variate_monte_carlo import \\\n    PolynomialModelEnsemble\nnp.random.seed(1)\n\npoly_model = PolynomialModelEnsemble()\nmodel_ensemble = pya.ModelEnsemble(poly_model.models)\ncov = poly_model.get_covariance_matrix()\ntarget_costs = np.array([1e1,1e2,1e3,1e4],dtype=int)\ncosts = np.asarray([10**-ii for ii in range(cov.shape[0])])\nmodel_labels=[r'$f_0$',r'$f_1$',r'$f_2$',r'$f_3$',r'$f_4$']\n\ndef plot_mlmc_error():\n    \"\"\"\n    Define function to create plot so we can create it later and add\n    other estimator error curves.\n\n    Note this is only necessary for sphinx-gallery docs. If just using the \n    jupyter notebooks they create then we do not need this function definition \n    and simply need to call fig at end of next cell to regenerate plot.\n    \"\"\"\n    variances, nsamples_history = [],[]\n    npilot_samples = 5\n    estimator = pya.MLMC\n    for target_cost in target_costs:\n        # compute variance  using exact covariance for sample allocation\n        est = estimator(cov,costs)\n        nhf_samples,nsample_ratios = est.allocate_samples(target_cost)[:2]\n        variances.append(est.get_variance(nhf_samples,nsample_ratios))\n        nsamples_history.append(est.get_nsamples(nhf_samples,nsample_ratios))\n        # compute single fidelity Monte Carlo variance\n        total_cost = nsamples_history[-1].dot(costs)\n        variances.append(cov[0,0]/int(total_cost/costs[0]))\n        nsamples_history.append(int(total_cost/costs[0]))\n        # compute variance using approx covariance for sample allocation\n        # use nhf_samples from previous target_cost as npilot_samples.\n        # This way the pilot samples are only an additional cost at the first\n        # step. This code does not do this though for simplicity\n        cov_approx = pya.estimate_model_ensemble_covariance(\n            npilot_samples,poly_model.generate_samples,model_ensemble)[0]\n        est = estimator(cov_approx,costs)\n        nhf_samples,nsample_ratios = est.allocate_samples(target_cost)[:2]\n        variances.append(est.get_variance(nhf_samples,nsample_ratios))\n        nsamples_history.append(est.get_nsamples(nhf_samples,nsample_ratios))\n        npilot_samples = nhf_samples\n\n    fig,axs=plt.subplots(1,2,figsize=(2*8,6))\n    pya.plot_acv_sample_allocation(nsamples_history[::3],costs,model_labels,axs[1])\n    total_costs = np.array(nsamples_history[::3]).dot(costs)\n    axs[0].loglog(total_costs,variances[::3],label=r'$\\mathrm{MLMC}$')\n    mc_line = axs[0].loglog(total_costs,variances[1::3],label=r'$\\mathrm{MC}$')\n    total_costs = np.array(nsamples_history[2::3]).dot(costs)\n    axs[0].loglog(total_costs,variances[2::3],'--',\n                  label=r'$\\mathrm{MLMC^\\dagger}$')\n    axs[0].set_xlabel(r'$\\mathrm{Total}\\;\\mathrm{Cost}$')\n    axs[0].set_ylabel(r'$\\mathrm{Variance}$')\n    _ = axs[0].legend()\n    return fig,axs\n\nfig,axs = plot_mlmc_error()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The left plot shows that the variance of the MLMC estimator is over and order of magnitude smaller than the variance of the single fidelity MC estimator for a fixed cost. The impact of using the approximate covariance is more significant for small samples sizes.\n\nThe right plot depicts the percentage of the computational cost due to evaluating each model. The numbers in the bars represent the number of samples allocated to each model. Relative to the low fidelity models only a small number of samples are allocated to the high-fidelity model, however evaluating these samples represents approximately 50\\% of the total cost.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multi-index Monte Carlo\n-----------------------\nMulti-Level Monte Carlo utilizes a sequence of models controlled by a single hyper-parameter, specifying the level of discretization for example, in a manner that balances computational cost with increasing accuracy. In many applications, however, multiple hyper-parameters may control the model discretization, such as the mesh and time step sizes. In these situations, it may not be clear how to construct a one-dimensional hierarchy represented by a scalar hyper-parameter. To overcome this limitation, a generalization of multi-level Monte Carlo, referred to as multi-index stochastic collocation (MIMC), was developed to deal with multivariate hierarchies with multiple refinement hyper-parameters [HNTNM2016]_.  PyApprox does not implement MIMC but a surrogate based version called Multi-index stochastic collocation (MISC) is presented in this tutorial `sphx_glr_auto_tutorials_multi_fidelity_plot_multi_index_collocation.py`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n^^^^^^^^^^\n.. [CGSTCVS2011] `K.A. Cliffe, M.B. Giles, R. Scheichl, A.L. Teckentrup, Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients, Comput. Vis. Sci., 14, 3-15, 2011. <https://doi.org/10.1007/s00791-011-0160-x>`_0\n\n.. [GOR2008] `M.B. Giles, Multilevel Monte Carlo path simulation, Oper. Res., 56(3), 607-617, 2008. <https://doi.org/10.1287/opre.1070.0496>`_\n\n.. [GAN2015] `M. Giles, Multilevel Monte Carlo methods, Acta Numerica, 24, 259-328, 2015. <https://doi.org/10.1017/S096249291500001X>`_\n\n.. [HNTNM2016] `A. Haji-Ali, F. Nobile and R. Tempone. Multi-index Monte Carlo: when sparsity meets sampling. Numerische Mathematik, 132(4), 767-806, 2016. <https://doi.org/10.1007/s00211-015-0734-5>`_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}